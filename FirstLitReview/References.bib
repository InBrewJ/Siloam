%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Jason  Brewer at 2017-06-13 12:43:52 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@article{chapelle_support_1999,
	Abstract = {Traditional classification approaches generalize poorly on image classification tasks, because of the high dimensionality of the feature space. This paper shows that support vector machines (SVM) can generalize well on difficult image classification problems where the only features are high dimensional histograms. Heavy-tailed RBF kernels of the form K(x, y)=e-ρΣi{\textbar}xia-yia{\textbar}b with a ⩽1 and b⩽2 are evaluated on the classification of images extracted from the Corel stock photo collection and shown to far outperform traditional polynomial or Gaussian radial basis function (RBF) kernels. Moreover, we observed that a simple remapping of the input xi→xia improves the performance of linear SVM to such an extend that it makes them, for this problem, a valid alternative to RBF kernels},
	Author = {Chapelle, O. and Haffner, P. and Vapnik, V. N.},
	Date-Added = {2017-06-13 10:43:18 +0000},
	Date-Modified = {2017-06-13 10:43:18 +0000},
	Doi = {10.1109/72.788646},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C93BAXGJ/788646.html:text/html},
	Issn = {1045-9227},
	Journal = {IEEE Transactions on Neural Networks},
	Keywords = {Classification tree analysis, Corel stock photo collection, feature space dimensionality, heavy-tailed RBF kernels, high-dimensional histograms, histogram-based image classification, Histograms, image classification, Image databases, Image recognition, Kernel, learning (artificial intelligence), linear SVM, Polynomials, radial basis function networks, remapping, Support vector machine classification, support vector machines, Web pages},
	Month = sep,
	Number = {5},
	Pages = {1055--1064},
	Title = {Support vector machines for histogram-based image classification},
	Volume = {10},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/72.788646}}

@inproceedings{ciregan_multi-column_2012,
	Abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	Author = {Ciregan, D. and Meier, U. and Schmidhuber, J.},
	Booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	Date-Added = {2017-06-13 10:43:18 +0000},
	Date-Modified = {2017-06-13 10:43:18 +0000},
	Doi = {10.1109/CVPR.2012.6248110},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C7UP7AGI/6248110.html:text/html},
	Keywords = {artificial neural network architectures, Benchmark testing, Computer architecture, computer vision, convolutional winner-take-all neurons, Error analysis, fast training, graphics cards, Graphics processing unit, graphics processing units, handwritten character recognition, handwritten digits recognition, human performance, image classification, Image recognition, learning (artificial intelligence), machine learning, MNIST handwriting benchmark, multicolumn deep neural networks, neural nets, Neurons, retina, sparsely connected neural layers, traffic sign recognition benchmark, traffic signs, Training, visual cortex},
	Month = jun,
	Pages = {3642--3649},
	Title = {Multi-column deep neural networks for image classification},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2012.6248110}}

@article{duro_comparison_2012,
	Abstract = {Pixel-based and object-based image analysis approaches for classifying broad land cover classes over agricultural landscapes are compared using three supervised machine learning algorithms: decision tree (DT), random forest (RF), and the support vector machine (SVM). Overall classification accuracies between pixel-based and object-based classifications were not statistically significant (p{\textgreater}0.05) when the same machine learning algorithms were applied. Using object-based image analysis, there was a statistically significant difference in classification accuracy between maps produced using the DT algorithm compared to maps produced using either RF (p=0.0116) or SVM algorithms (p=0.0067). Using pixel-based image analysis, there was no statistically significant difference (p{\textgreater}0.05) between results produced using different classification algorithms. Classifications based on RF and SVM algorithms provided a more visually adequate depiction of wetland, riparian, and crop land cover types when compared to DT based classifications, using either object-based or pixel-based image analysis. In this study, pixel-based classifications utilized fewer variables (15 vs. 300), achieved similar classification accuracies, and required less time to produce than object-based classifications. Object-based classifications produced a visually appealing generalized appearance of land cover classes. Based exclusively on overall accuracy reports, there was no advantage to preferring one image analysis approach over another for the purposes of mapping broad land cover types in agricultural environments using medium spatial resolution earth observation imagery.},
	Author = {Duro, Dennis C. and Franklin, Steven E. and Dub{\'e}, Monique G.},
	Date-Added = {2017-06-13 10:43:18 +0000},
	Date-Modified = {2017-06-13 10:43:18 +0000},
	Doi = {10.1016/j.rse.2011.11.020},
	File = {ScienceDirect Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/NR23EKKS/S0034425711004172.html:text/html},
	Issn = {0034-4257},
	Journal = {Remote Sensing of Environment},
	Keywords = {Comparison, Decision tree, Object-based, Random forest, Support vector machine},
	Month = mar,
	Pages = {259--272},
	Title = {A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using {SPOT}-5 {HRG} imagery},
	Url = {http://www.sciencedirect.com/science/article/pii/S0034425711004172},
	Volume = {118},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0034425711004172},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.rse.2011.11.020}}

@inproceedings{zollner_navi_2011,
	Abstract = {We present a proof-of-concept of a mobile navigational aid that uses the Microsoft Kinect and optical marker tracking to help visually impaired people find their way inside buildings. The system is the result of a student project and is entirely based on low-cost hard- and software. It provides continuous vibrotactile feedback on the person's waist, to give an impression of the environment and to warn about obstacles. Furthermore, optical markers can be used to tag points of interest within the building to enable synthesized voice instructions for point-to-point navigation.},
	Author = {Z{\"o}llner, Michael and Huber, Stephan and Jetter, Hans-Christian and Reiterer, Harald},
	Booktitle = {Human-{Computer} {Interaction} -- {INTERACT} 2011},
	Date-Added = {2017-06-13 10:04:52 +0000},
	Date-Modified = {2017-06-13 10:04:52 +0000},
	Doi = {10.1007/978-3-642-23768-3_88},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/GT4Q48VQ/Z{\"o}llner et al. - 2011 - NAVI -- A Proof-of-Concept of a Mobile Navigational.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/JKS8HB44/10.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {584--587},
	Publisher = {Springer, Berlin, Heidelberg},
	Title = {{NAVI} -- {A} {Proof}-of-{Concept} of a {Mobile} {Navigational} {Aid} for {Visually} {Impaired} {Based} on the {Microsoft} {Kinect}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-642-23768-3_88},
	Urldate = {2017-06-13},
	Year = {2011},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-642-23768-3_88},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-642-23768-3_88}}

@inproceedings{quinones_supporting_2011,
	Abstract = {In this paper, we investigate the requirements for designing systems to support wayfinding for visually impaired individuals. We report the results of an interview study with 20 individuals with visual impairments, asking about their way-finding tools, techniques, and obstacles. Our findings provide an account of the practices followed when navigating familiar, unfamiliar, and dynamic environments, and common breakdowns encountered during the wayfinding process. The findings from this study suggest ways of implementing a location-based system to assist in the recovery from various obstacles.},
	Address = {New York, NY, USA},
	Author = {Quinones, Pablo-Alejandro and Greene, Tammy and Yang, Rayoung and Newman, Mark},
	Booktitle = {{CHI} '11 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	Date-Added = {2017-06-13 08:37:30 +0000},
	Date-Modified = {2017-06-13 08:37:30 +0000},
	Doi = {10.1145/1979742.1979822},
	Isbn = {978-1-4503-0268-5},
	Keywords = {Accessibility, location-based computing, visual impairment, wayfinding},
	Pages = {1645--1650},
	Publisher = {ACM},
	Series = {{CHI} {EA} '11},
	Shorttitle = {Supporting {Visually} {Impaired} {Navigation}},
	Title = {Supporting {Visually} {Impaired} {Navigation}: {A} {Needs}-finding {Study}},
	Url = {http://doi.acm.org/10.1145/1979742.1979822},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1979742.1979822},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1979742.1979822}}

@article{panicker_design_2016,
	Author = {Panicker, Sanjana and KV, Maitreyi and Gonsalves, Merrill and Gonsalves, Kane and Anand, Sandeep and Pati, Sandhya},
	Issn = {2321-7782},
	Journal = {International Journal of Advance Research in Computer Science and Management Studies},
	Month = oct,
	Number = {10},
	Pages = {90--98},
	Title = {Design and {Overview} of a {Navigation} {Application} for the {Blind}},
	Url = {http://www.ijarcsms.com/docs/paper/volume4/issue10/V4I10-0017.pdf},
	Volume = {4},
	Year = {2016},
	Bdsk-Url-1 = {http://www.ijarcsms.com/docs/paper/volume4/issue10/V4I10-0017.pdf}}

@article{guerrero_indoor_2012,
	Abstract = {Navigation in indoor environments is highly challenging for the severely visually impaired, particularly in spaces visited for the first time. Several solutions have been proposed to deal with this challenge. Although some of them have shown to be useful in real scenarios, they involve an important deployment effort or use artifacts that are not natural for blind users. This paper presents an indoor navigation system that was designed taking into consideration usability as the quality requirement to be maximized. This solution enables one to identify the position of a person and calculates the velocity and direction of his movements. Using this information, the system determines the user's trajectory, locates possible obstacles in that route, and offers navigation information to the user. The solution has been evaluated using two experimental scenarios. Although the results are still not enough to provide strong conclusions, they indicate that the system is suitable to guide visually impaired people through an unknown built environment.},
	Author = {Guerrero, Luis A. and Vasquez, Francisco and Ochoa, Sergio F.},
	Copyright = {http://creativecommons.org/licenses/by/3.0/},
	Doi = {10.3390/s120608236},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/N6NDPMBT/Guerrero et al. - 2012 - An Indoor Navigation System for the Visually Impai.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/N473J39S/htm.html:text/html},
	Journal = {Sensors},
	Keywords = {augmented object, indoor positioning, movement detection, navigation system, visually impaired, voice interface},
	Language = {en},
	Month = jun,
	Number = {6},
	Pages = {8236--8258},
	Title = {An {Indoor} {Navigation} {System} for the {Visually} {Impaired}},
	Url = {http://www.mdpi.com/1424-8220/12/6/8236},
	Urldate = {2017-06-13},
	Volume = {12},
	Year = {2012},
	Bdsk-Url-1 = {http://www.mdpi.com/1424-8220/12/6/8236},
	Bdsk-Url-2 = {http://dx.doi.org/10.3390/s120608236}}

@article{bradley_experimental_2005,
	Abstract = {In recent years, there has been an escalation of orientation and wayfinding technologies and systems for visually impaired people. These technological advancements, however, have not been matched by a suitable investigation of human-computer interaction (e.g. designing navigation aids for people who form different cognitive maps for navigation). The aim of this study is to investigate whether a group of sighted participants and a group of visually impaired participants experience a difference in mental and physical demands when given two different sets of verbal instructions directing them to four landmarks. The content of the first set of instructions was proportioned to route descriptions derived from sighted people, and the second set proportioned to descriptions derived from visually impaired people. The objective assessment involved measuring the time taken by participants to reach landmarks and the number of deviations that occurred. A NASA--Task Load Index questionnaire provided an indication of participants subjective perception of workload. The results revealed that instructions formed from visually impaired people resulted in a lower weighted workload score, less minor deviations, and quicker times for visually impaired participants. In contrast, these instructions were found to cause a higher weighted workload score for sighted participants. The results are discussed in relation to the issue of personalisation of mobile context-aware systems for visually impaired people.},
	Author = {Bradley, Nicholas A. and Dunlop, Mark D.},
	Doi = {10.1007/s00779-005-0350-y},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/ZHBXCXXQ/s00779-005-0350-y.html:text/html},
	Issn = {1617-4909, 1617-4917},
	Journal = {Pers Ubiquit Comput},
	Language = {en},
	Month = nov,
	Number = {6},
	Pages = {395--403},
	Title = {An {Experimental} {Investigation} into {Wayfinding} {Directions} for {Visually} {Impaired} {People}},
	Url = {https://link.springer.com/article/10.1007/s00779-005-0350-y},
	Urldate = {2017-06-13},
	Volume = {9},
	Year = {2005},
	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s00779-005-0350-y},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s00779-005-0350-y}}

@article{serrao_navigation_2014,
	Abstract = {In an unfamiliar environment we spot and explore all available information which might guide us to a desired location. This largely unconscious processing is done by our trained sensory and cognitive systems. These recognise and memorise sets of landmarks which allow us to create a mental map of the environment, and this map enables us to navigate by exploiting very few but the most important landmarks stored in our memory. In this paper we present a route planning, localisation and navigation system which works in real time. It integrates a geographic information system of a building with visual landmarks for localising the user and for validating the navigation route. Although designed for visually impaired persons, the system can also be employed to assist or transport persons with reduced mobility in way finding in a complex building.},
	Author = {Serr{\~a}o, M. and Rodrigues, J. M. F. and Buf, J. M. H. du},
	Doi = {10.1016/j.procs.2014.02.005},
	File = {ScienceDirect Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/KTV26T43/Serr{\~a}o et al. - 2014 - Navigation Framework Using Visual Landmarks and a .pdf:application/pdf;ScienceDirect Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/99DD6IDK/S1877050914000076.html:text/html},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Accessibility, Geographic information system, Navigation, Vision, Visually impaired persons},
	Month = jan,
	Pages = {28--37},
	Series = {5th {International} {Conference} on {Software} {Development} and {Technologies} for {Enhancing} {Accessibility} and {Fighting} {Info}-exclusion, {DSAI} 2013},
	Title = {Navigation {Framework} {Using} {Visual} {Landmarks} and a {GIS}},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050914000076},
	Volume = {27},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050914000076},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.procs.2014.02.005}}

@inproceedings{schmitz_acquisition_2011,
	Abstract = {In order to allow blind people independent navigation in unknown areas, we have developed a navigation system that seamlessly integrates both static maps and dynamic location-based textual information from a variety of sources. Each information source requires a different kind of acquisition technique. The acquired information is integrated by a context management platform and then presented to the user on a tactile-acoustical map depending on the sources available for his current position. Positioning is achieved by a combination of an inertial tracking system, RFID technology and GPS and the user is guided to a desired destination by speech output and a hap tic cane. The resulting system is the first of its kind to integrate a variety of maps and other accumulated location-based information on a unified interface for blind people.},
	Author = {Schmitz, B. and Becker, S. and Blessing, A. and Gro{\ss}mann, M.},
	Booktitle = {2011 {IEEE} 12th {International} {Conference} on {Mobile} {Data} {Management}},
	Doi = {10.1109/MDM.2011.66},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/V6SP4ZK8/6068447.html:text/html},
	Keywords = {acoustic signal processing, acquired information, acquisition technique, blind navigation, blind people independent navigation, Buildings, cartography, Context, context management platform, data acquisition, data presentation, diverse spatial context data, dynamic location-based textual information, Global Positioning System, GPS, handicapped aids, haptic cane, haptic interfaces, inertial tracking system, information source, location-based information, mobile computing, navigation system, RFID technology, Servers, spatial data structures, speech output, static maps, tactile-acoustical map, Three dimensional displays, unified interface, Vibrations},
	Month = jun,
	Pages = {276--284},
	Title = {Acquisition and {Presentation} of {Diverse} {Spatial} {Context} {Data} for {Blind} {Navigation}},
	Volume = {1},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/MDM.2011.66}}

@misc{noauthor_indooratlas_nodate,
	Author = {University of Oulu},
	Date-Modified = {2017-06-13 07:31:26 +0000},
	File = {IndoorAtlas pioneers its location technology with passion | Computer Science and Engineering:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/MUWPPDXM/17779.html:text/html},
	Title = {{IndoorAtlas} pioneers its location technology with passion {\textbar} {Computer} {Science} and {Engineering}},
	Url = {http://www.oulu.fi/cse/node/17779},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://www.oulu.fi/cse/node/17779}}

@misc{noauthor_seeing_nodate,
	Abstract = {Pivothead Wearable Imaging is a pioneer in smart eyewear and wearable cameras, and provides enterprise solutions around POV video, photo, live streaming, and video VoIP conferencing calls, all offered as separate applications on our visual communications IOT platform.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/SKUTKGMZ/seeingai.html:text/html},
	Journal = {Pivothead Wearable Imaging},
	Title = {Seeing {AI} {Project}},
	Url = {http://www.pivothead.com/seeingai/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://www.pivothead.com/seeingai/}}

@misc{noauthor_openstreetmap_nodate,
	Abstract = {OpenStreetMap is a map of the world, created by people like you and free to use under an open licence.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/FJ3KXVSU/www.openstreetmap.org.html:text/html},
	Journal = {OpenStreetMap},
	Title = {{OpenStreetMap}},
	Url = {https://www.openstreetmap.org/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://www.openstreetmap.org/}}

@misc{noauthor_google_nodate,
	Abstract = {Embed real-world imagery with 360-degree panoramas using the Google Street View Image API.},
	Author = {Google},
	Date-Modified = {2017-06-13 10:33:08 +0000},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/4IBCBNXA/streetview.html:text/html},
	Journal = {Google Developers},
	Title = {Google {Street} {View} {Image} {API}},
	Url = {https://developers.google.com/maps/documentation/streetview/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://developers.google.com/maps/documentation/streetview/}}

@misc{noauthor_getting_nodate,
	Abstract = {Tango adds extra sensors to an Android device for computer vision and depth perception. It can learn its surroundings visually and understand its position in 3D space. The C API gives developers native access to the device's sensor information.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/Z9I6WZV6/c.html:text/html},
	Journal = {Google Developers},
	Title = {Getting {Started} with the {Tango} {C} {API} {\textbar} {Tango} {C} {API}},
	Url = {https://developers.google.com/tango/apis/c/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://developers.google.com/tango/apis/c/}}

@inproceedings{song_sliding_2014,
	Abstract = {The depth information of RGB-D sensors has greatly simplified some common challenges in computer vision and enabled breakthroughs for several tasks. In this paper, we propose to use depth maps for object detection and design a 3D detector to overcome the major difficulties for recognition, namely the variations of texture, illumination, shape, viewpoint, clutter, occlusion, self-occlusion and sensor noises. We take a collection of 3D CAD models and render each CAD model from hundreds of viewpoints to obtain synthetic depth maps. For each depth rendering, we extract features from the 3D point cloud and train an Exemplar-SVM classifier. During testing and hard-negative mining, we slide a 3D detection window in 3D space. Experiment results show that our 3D detector significantly outperforms the state-of-the-art algorithms for both RGB and RGB-D images, and achieves about ×1.7 improvement on average precision compared to DPM and R-CNN. All source code and data are available online.},
	Author = {Song, Shuran and Xiao, Jianxiong},
	Booktitle = {Computer {Vision} -- {ECCV} 2014},
	Doi = {10.1007/978-3-319-10599-4_41},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/TXNCW9K5/978-3-319-10599-4_41.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {634--651},
	Publisher = {Springer, Cham},
	Title = {Sliding {Shapes} for 3D {Object} {Detection} in {Depth} {Images}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_41},
	Urldate = {2017-06-06},
	Year = {2014},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_41},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-10599-4_41}}

@misc{marburg_opencv-ffi-ext:_2017,
	Author = {Marburg, Aaron},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/MGMRFM7N/opensurf.html:text/html},
	Month = mar,
	Note = {original-date: 2012-05-07T07:29:38Z},
	Shorttitle = {opencv-ffi-ext},
	Title = {opencv-ffi-ext: {Native} compiled extensions to {Opencv}-{FFI}},
	Url = {https://github.com/amarburg/opencv-ffi-ext},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/amarburg/opencv-ffi-ext}}

@article{lingua_performance_2009,
	Abstract = {In the photogrammetry field, interest in region detectors, which are widely used in Computer Vision, is quickly increasing due to the availability of new techniques. Images acquired by Mobile Mapping Technology, Oblique Photogrammetric Cameras or Unmanned Aerial Vehicles do not observe normal acquisition conditions. Feature extraction and matching techniques, which are traditionally used in photogrammetry, are usually inefficient for these applications as they are unable to provide reliable results under extreme geometrical conditions (convergent taking geometry, strong affine transformations, etc.) and for bad-textured images. A performance analysis of the SIFT technique in aerial and close-range photogrammetric applications is presented in this paper. The goal is to establish the suitability of the SIFT technique for automatic tie point extraction and approximate DSM (Digital Surface Model) generation. First, the performances of the SIFT operator have been compared with those provided by feature extraction and matching techniques used in photogrammetry. All these techniques have been implemented by the authors and validated on aerial and terrestrial images. Moreover, an auto-adaptive version of the SIFT operator has been developed, in order to improve the performances of the SIFT detector in relation to the texture of the images. The Auto-Adaptive SIFT operator (A2 SIFT) has been validated on several aerial images, with particular attention to large scale aerial images acquired using mini-UAV systems.},
	Author = {Lingua, Andrea and Marenchino, Davide and Nex, Francesco},
	Copyright = {http://creativecommons.org/licenses/by/3.0/},
	Doi = {10.3390/s90503745},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/D2S54BJG/Lingua et al. - 2009 - Performance Analysis of the SIFT Operator for Auto.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/5MJ3D9CA/3745.html:text/html},
	Journal = {Sensors},
	Keywords = {feature extraction, feature matching, image orientation, location accuracy, SIFT operator},
	Language = {en},
	Month = may,
	Number = {5},
	Pages = {3745--3766},
	Title = {Performance {Analysis} of the {SIFT} {Operator} for {Automatic} {Feature} {Extraction} and {Matching} in {Photogrammetric} {Applications}},
	Url = {http://www.mdpi.com/1424-8220/9/5/3745},
	Urldate = {2017-06-12},
	Volume = {9},
	Year = {2009},
	Bdsk-Url-1 = {http://www.mdpi.com/1424-8220/9/5/3745},
	Bdsk-Url-2 = {http://dx.doi.org/10.3390/s90503745}}

@inproceedings{jafri_obstacle_2016,
	Abstract = {A depth-data based obstacle detection and avoidance application for VI users to assist them in navigating independently in previously unmapped indoors environments is presented. The application is being developed for the recently introduced Google Project Tango Tablet Development Kit equipped with a powerful processor (NVIDIA Tegra K1 with 192 CUDA cores) as well as var-ious sensors which allow it track its motion and orientation in 3D space in real-time. Depth data for the area in front of the users, obtained using the tablet's in-built infrared--based depth sensor, is analyzed to detect obstacles and audio-based navigation instructions are provided accordingly. A visual display option is also offered for users with low vision. The aim is to develop a real-time, affordable, aesthetically acceptable, mobile assistive stand-alone application on a cutting-edge device, adopting a user-centered approach, which allows VI users to micro-navigate autonomously in possibly unfamiliar indoor surroundings.},
	Author = {Jafri, Rabia and Khan, Marwa Mahmoud},
	Booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	Doi = {10.1007/978-3-319-41267-2_24},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/6S7A8GSS/978-3-319-41267-2_24.html:text/html},
	Language = {en},
	Month = jul,
	Pages = {179--185},
	Publisher = {Springer, Cham},
	Title = {Obstacle {Detection} and {Avoidance} for the {Visually} {Impaired} in {Indoors} {Environments} {Using} {Google}'s {Project} {Tango} {Device}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-41267-2_24},
	Urldate = {2017-06-06},
	Year = {2016},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-41267-2_24},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-41267-2_24}}

@book{sonka_image_2014,
	Abstract = {The brand new edition of IMAGE PROCESSING, ANALYSIS, AND MACHINE VISION is a robust text providing deep and wide coverage of the full range of topics encountered in the field of image processing and machine vision. As a result, it can serve undergraduates, graduates, researchers, and professionals looking for a readable reference. The book's encyclopedic coverage of topics is wide, and it can be used in more than one course (both image processing and machine vision classes). In addition, while advanced mathematics is not needed to understand basic concepts (making this a good choice for undergraduates), rigorous mathematical coverage is included for more advanced readers. It is also distinguished by its easy-to-understand algorithm descriptions of difficult concepts, and a wealth of carefully selected problems and examples.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
	Author = {Sonka, Milan and Hlavac, Vaclav and Boyle, Roger},
	Isbn = {978-1-285-98144-4},
	Keywords = {Technology \& Engineering / Engineering (General)},
	Language = {en},
	Month = jan,
	Note = {Google-Books-ID: QePKAgAAQBAJ},
	Publisher = {Cengage Learning},
	Title = {Image {Processing}, {Analysis}, and {Machine} {Vision}},
	Year = {2014}}

@misc{evans_opensurf_2015,
	Author = {Evans, Chris},
	File = {OpenSURF - The Official Home of the Image Processing Library:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/7DX3KKTZ/computer-vision-opensurf.html:text/html},
	Month = feb,
	Title = {{OpenSURF} - {The} {Official} {Home} of the {Image} {Processing} {Library}},
	Url = {https://web.archive.org/web/20150225125650/http://www.chrisevansdev.com/computer-vision-opensurf.html},
	Year = {2015},
	Bdsk-Url-1 = {https://web.archive.org/web/20150225125650/http://www.chrisevansdev.com/computer-vision-opensurf.html}}

@misc{noauthor_visualsfm_nodate,
	File = {VisualSFM \: A Visual Structure from Motion System:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/PWPR9XJQ/vsfm.html:text/html},
	Title = {{VisualSFM} : {A} {Visual} {Structure} from {Motion} {System}},
	Url = {http://ccwu.me/vsfm/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://ccwu.me/vsfm/}}

@misc{noauthor_pmvs2_nodate,
	File = {PMVS2:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3WAEK42U/pmvs.html:text/html},
	Title = {{PMVS}2},
	Url = {http://www.di.ens.fr/pmvs/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://www.di.ens.fr/pmvs/}}

@misc{noauthor_cydalion._nodate,
	File = {Cydalion. Navigation App for the Blind and Visually Impaired:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/F2ZV58DB/cydalion.com.html:text/html},
	Title = {Cydalion. {Navigation} {App} for the {Blind} and {Visually} {Impaired}},
	Url = {http://cydalion.com/#features},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://cydalion.com/#features}}

@misc{author_ariadne_nodate,
	Abstract = {There are hundreds of gps apps},
	Author = {Author, AppAdvice Staff},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/IMQVJMN8/441063072.html:text/html},
	Journal = {AppAdvice},
	Title = {Ariadne {GPS} by {Giovanni} {Ciaffoni}},
	Url = {/app/ariadne-gps/441063072},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {/app/ariadne-gps/441063072}}

@misc{noauthor_ariane_nodate,
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C67QG4N2/how.html:text/html},
	Journal = {Mability},
	Title = {Ariane {Project}},
	Url = {http://arianeproject.com/how/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://arianeproject.com/how/}}

@misc{noauthor_tango_nodate,
	Abstract = {See more of your world.},
	Author = {Google},
	Date-Modified = {2017-06-13 07:59:13 +0000},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/6P5Q2S2W/tango.html:text/html},
	Journal = {Tango},
	Title = {Tango},
	Url = {https://get.google.com/tango/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://get.google.com/tango/}}

@inproceedings{riehle_indoor_2008,
	Abstract = {Indoor navigation technology is needed to support seamless mobility for the visually impaired. A small portable personal navigation device that provides current position, useful contextual wayfinding information about the indoor environment and directions to a destination would greatly improve access and independence for people with low vision. This paper describes the construction of such a device which utilizes a commercial Ultra-Wideband (UWB) asset tracking system to support real-time location and navigation information. Human trials were conducted to assess the efficacy of the system by comparing target-finding performance between blindfolded subjects using the navigation system for real-time guidance, and blindfolded subjects who only received speech information about their local surrounds but no route guidance information (similar to that available from a long cane or guide dog). A normal vision control condition was also run. The time and distance traveled was measured in each trial and a point-back test was performed after goal completion to assess cognitive map development. Statistically significant differences were observed between the three conditions in time and distance traveled; with the navigation system and the visual condition yielding the best results, and the navigation system dramatically outperforming the non-guided condition.},
	Author = {Riehle, T. H. and Lichter, P. and Giudice, N. A.},
	Booktitle = {2008 30th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}},
	Doi = {10.1109/IEMBS.2008.4650195},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/35HIKIE7/4650195.html:text/html},
	Keywords = {Algorithms, Biomedical Engineering, Blindness, Canes, Equipment Design, Humans, Indoor environments, Microcomputers, Navigation, Performance evaluation, Real time systems, Sensory Aids, Software, Speech, Target tracking, Testing, Time measurement, Ultra wideband technology, User-Computer Interface, Visual Fields, Visually impaired persons, Walking},
	Month = aug,
	Pages = {4435--4438},
	Title = {An indoor navigation system to support the visually impaired},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IEMBS.2008.4650195}}

@inproceedings{sonnenblick1998indoor,
	Author = {Sonnenblick, Yehuda},
	Booktitle = {Proceedings of the 13th annual Conference on Technology and Persons with Disabilities},
	Pages = {215--224},
	Title = {An indoor navigation system for blind individuals},
	Year = {1998}}

@inproceedings{schmitz2010viibracane,
	Author = {Schmitz, Bernhard},
	Booktitle = {Proceedings of the California State University, Northridge Center on Disabilities' 25th Annual International Technology and Persons with Disabilities Conference (CSUN 2010), Los Angeles, CA, USA},
	Title = {The ViibraCane-A white cane for tactile navigation guidance},
	Year = {2010}}
