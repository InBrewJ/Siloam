%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Jason  Brewer at 2017-06-30 19:47:27 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{noauthor_cpprestsdk:_2017,
	Date-Added = {2017-06-30 11:36:45 +0000},
	Date-Modified = {2017-06-30 11:36:45 +0000},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/EIKT22VJ/cpprestsdk.html:text/html},
	Keywords = {async, asynchronous-tasks, cloud, cpp, cpp11, cpp-library, http, oauth, oauth2, sdk, tasks, websockets},
	Month = jun,
	Note = {original-date: 2014-08-21T20:57:45Z},
	Publisher = {Microsoft},
	Shorttitle = {cpprestsdk},
	Title = {cpprestsdk: {The} {C}++ {REST} {SDK} is a {Microsoft} project for cloud-based client-server communication in native code using a modern asynchronous {C}++ {API} design. {This} project aims to help {C}++ developers ..},
	Url = {https://github.com/Microsoft/cpprestsdk},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/Microsoft/cpprestsdk}}

@article{gonzalesdigital,
	Author = {Gonzales, RC},
	Date-Added = {2017-06-30 09:38:08 +0000},
	Date-Modified = {2017-06-30 09:39:32 +0000},
	Journal = {Maury D. Correia},
	Title = {Digital image processing 3rd Edition, 2007},
	Year = {2007}}

@misc{cignoni_meshlab:_2008,
	Abstract = {Eurographics Italian Chapter Conference},
	Author = {Cignoni, Paolo and Callieri, Marco and Corsini, Massimiliano and Dellepiane, Matteo and Ganovelli, Fabio and Ranzuglia, Guido},
	Date-Added = {2017-06-30 09:26:33 +0000},
	Date-Modified = {2017-06-30 09:26:33 +0000},
	Language = {eng},
	Note = {DOI: 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136},
	Publisher = {The Eurographics Association},
	Shorttitle = {{MeshLab}},
	Title = {{MeshLab}: an {Open}-{Source} {Mesh} {Processing} {Tool}},
	Year = {2008}}

@article{chang_libsvm:_2011,
	Abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.},
	Author = {Chang, Chih-Chung and Lin, Chih-Jen},
	Date-Added = {2017-06-29 16:17:00 +0000},
	Date-Modified = {2017-06-29 16:17:00 +0000},
	Doi = {10.1145/1961189.1961199},
	Issn = {2157-6904},
	Journal = {ACM Trans. Intell. Syst. Technol.},
	Keywords = {Classification, LIBSVM, machines, Optimization, regression, support, SVM, vector},
	Month = may,
	Number = {3},
	Pages = {27:1--27:27},
	Shorttitle = {{LIBSVM}},
	Title = {{LIBSVM}: {A} {Library} for {Support} {Vector} {Machines}},
	Url = {http://doi.acm.org/10.1145/1961189.1961199},
	Volume = {2},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1961189.1961199},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1961189.1961199}}

@inproceedings{labbe_online_2014,
	Abstract = {For large-scale and long-term simultaneous localization and mapping (SLAM), a robot has to deal with unknown initial positioning caused by either the kidnapped robot problem or multi-session mapping. This paper addresses these problems by tying the SLAM system with a global loop closure detection approach, which intrinsically handles these situations. However, online processing for global loop closure detection approaches is generally influenced by the size of the environment. The proposed graph-based SLAM system uses a memory management approach that only consider portions of the map to satisfy online processing requirements. The approach is tested and demonstrated using five indoor mapping sessions of a building using a robot equipped with a laser rangefinder and a Kinect.},
	Author = {Labb{\'e}, M. and Michaud, F.},
	Booktitle = {2014 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	Date-Added = {2017-06-29 14:36:32 +0000},
	Date-Modified = {2017-06-29 14:36:32 +0000},
	Doi = {10.1109/IROS.2014.6942926},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3JJ428B3/6942926.html:text/html},
	Keywords = {graph theory, indoor mapping sessions, initial positioning, kidnapped robot problem, Kinect, large-scale multisession graph-based SLAM, laser rangefinder, laser ranging, Lasers, Memory management, memory management approach, mobile robots, multisession mapping, online global loop closure detection, online processing requirements, Optimization, position control, robot vision, simultaneous localization and mapping, SLAM (robots), Three-dimensional displays, Visualization},
	Month = sep,
	Pages = {2661--2666},
	Title = {Online global loop closure detection for large-scale multi-session graph-based {SLAM}},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IROS.2014.6942926}}

@inproceedings{labbe_memory_2011,
	Abstract = {Loop closure detection is the process involved when trying to find a match between the current and a previously visited locations in SLAM. Over time, the amount of time required to process new observations increases with the size of the internal map, which may influence real-time processing. In this paper, we present a novel real-time loop closure detection approach for large-scale and long-term SLAM. Our approach is based on a memory management method that keeps computation time for each new observation under a fixed limit. Results demonstrate the approach's adaptability and scalability using four standard data sets.},
	Author = {Labb{\'e}, M. and Michaud, F.},
	Booktitle = {2011 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems}},
	Date-Added = {2017-06-29 14:36:32 +0000},
	Date-Modified = {2017-06-29 14:36:32 +0000},
	Doi = {10.1109/IROS.2011.6094602},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/VBGFCP9P/6094602.html:text/html},
	Keywords = {Bayesian methods, Dictionaries, feature extraction, image matching, internal map, location matching, Memory management, mobile robots, real-time appearance, real-time loop closure detection approach, Real time systems, robot vision, simultaneous localization and mapping, SLAM, SLAM (robots), storage management, Visualization},
	Month = sep,
	Pages = {1271--1276},
	Title = {Memory management for real-time appearance-based loop closure detection},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IROS.2011.6094602}}

@misc{noauthor_rtabmap:_2017,
	Copyright = {BSD-3-Clause},
	Date-Added = {2017-06-29 14:36:32 +0000},
	Date-Modified = {2017-06-29 14:36:32 +0000},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/48II6H4U/rtabmap.html:text/html},
	Keywords = {localization, mapping, project-tango, robotics, scanning, SLAM},
	Month = jun,
	Note = {original-date: 2014-08-11T18:38:14Z},
	Publisher = {IntRoLab},
	Shorttitle = {rtabmap},
	Title = {rtabmap: {RTAB}-{Map} library and standalone application},
	Url = {https://github.com/introlab/rtabmap},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/introlab/rtabmap}}

@misc{noauthor_street_nodate,
	Date-Added = {2017-06-29 14:01:56 +0000},
	Date-Modified = {2017-06-29 14:01:56 +0000},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/Z5EKRAT6/getting-started.html:text/html},
	Journal = {Google Developers},
	Title = {Street {View} {Publish} {API} {Overview} {\textbar} {Street} {View} {Publish} {API}},
	Url = {https://developers.google.com/streetview/publish/getting-started},
	Urldate = {2017-06-29},
	Bdsk-Url-1 = {https://developers.google.com/streetview/publish/getting-started}}

@article{choset_topological_2001,
	Abstract = {This paper presents a new method for simultaneous localization and mapping that exploits the topology of the robot's free space to localize the robot on a partially constructed map. The topology of the environment is encoded in a topological map; the particular topological map used in this paper is the generalized Voronoi graph (GVG), which also encodes some metric information about the robot's environment, as well. In this paper, we present the low-level control laws that generate the GVG edges and nodes, thereby allowing for exploration of an unknown space. With these prescribed control laws, the GVG can be viewed as an arbitrator for a hybrid control system that determines when to invoke a particular low-level controller from a set of controllers all working toward the high-level capability of mobile robot exploration. The main contribution, however, is using the graph structure of the GVG, via a graph matching process, to localize the robot. Experimental results verify the described work},
	Author = {Choset, H. and Nagatani, K.},
	Date-Added = {2017-06-29 08:16:38 +0000},
	Date-Modified = {2017-06-29 08:16:38 +0000},
	Doi = {10.1109/70.928558},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3M4SE46M/928558.html:text/html},
	Issn = {1042-296X},
	Journal = {IEEE Transactions on Robotics and Automation},
	Keywords = {computational geometry, Control systems, Global Positioning System, graph matching, Mechanical engineering, mobile robot, mobile robots, motion planning, Navigation, Orbital robotics, path planning, pattern matching, position control, robot free space, Robotics and Automation, Robot kinematics, simultaneous localization, simultaneous localization and mapping, topological map, topology, Voronoi graph, Wheels},
	Month = apr,
	Number = {2},
	Pages = {125--137},
	Shorttitle = {Topological simultaneous localization and mapping ({SLAM})},
	Title = {Topological simultaneous localization and mapping ({SLAM}): toward exact localization without explicit localization},
	Volume = {17},
	Year = {2001},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/70.928558}}

@inproceedings{deng_imagenet:_2009,
	Abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	Author = {Deng, J. and Dong, W. and Socher, R. and Li, L. J. and Li, Kai and Fei-Fei, Li},
	Booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	Date-Added = {2017-06-29 07:46:15 +0000},
	Date-Modified = {2017-06-29 07:46:15 +0000},
	Doi = {10.1109/CVPR.2009.5206848},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/NJBA2WA2/5206848.html:text/html},
	Keywords = {computer vision, Explosions, Image databases, ImageNet database, image resolution, image retrieval, Information retrieval, Internet, large-scale hierarchical image database, large-scale ontology, Large-scale systems, multimedia computing, multimedia data, Multimedia databases, Ontologies, ontologies (artificial intelligence), Robustness, Spine, subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	Month = jun,
	Pages = {248--255},
	Shorttitle = {{ImageNet}},
	Title = {{ImageNet}: {A} large-scale hierarchical image database},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2009.5206848}}

@misc{noauthor_princeton_nodate,
	Date-Added = {2017-06-29 07:46:15 +0000},
	Date-Modified = {2017-06-29 07:46:15 +0000},
	File = {Princeton ModelNet:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3VRQ6RR8/modelnet.cs.princeton.edu.html:text/html},
	Title = {Princeton {ModelNet}},
	Url = {http://modelnet.cs.princeton.edu/},
	Urldate = {2017-06-29},
	Bdsk-Url-1 = {http://modelnet.cs.princeton.edu/}}

@misc{trimble_inc._3d_nodate,
	Author = {Trimble Inc.},
	Date-Added = {2017-06-28 08:17:40 +0000},
	Date-Modified = {2017-06-28 08:17:40 +0000},
	File = {3D Warehouse:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/FRFFC92E/3dwarehouse.sketchup.com.html:text/html},
	Title = {3D {Warehouse}},
	Url = {https://3dwarehouse.sketchup.com/?hl=en},
	Urldate = {2017-06-28},
	Bdsk-Url-1 = {https://3dwarehouse.sketchup.com/?hl=en}}

@inproceedings{browatzki_going_2011,
	Abstract = {Categorization of objects solely based on shape and appearance is still a largely unresolved issue. With the advent of new sensor technologies, such as consumer-level range sensors, new possibilities for shape processing have become available for a range of new application domains. In the first part of this paper, we introduce a novel, large dataset containing 18 categories of objects found in typical household and office environments-we envision this dataset to be useful in many applications ranging from robotics to computer vision. The second part of the paper presents computational experiments on object categorization with classifiers exploiting both two-dimensional and three-dimensional information. We evaluate categorization performance for both modalities in separate and combined representations and demonstrate the advantages of using range data for object and shape processing skills.},
	Author = {Browatzki, B. and Fischer, J. and Graf, B. and B{\"u}lthoff, H. H. and Wallraven, C.},
	Booktitle = {2011 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCV} {Workshops})},
	Date-Added = {2017-06-28 08:00:03 +0000},
	Date-Modified = {2017-06-28 08:00:03 +0000},
	Doi = {10.1109/ICCVW.2011.6130385},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/SR6GR5AH/6130385.html:text/html},
	Keywords = {computer vision, consumer-level range sensors, Databases, image classification, Image color analysis, large-scale object dataset, Liquids, object categorization, object classification, object detection, robotics, Robot sensing systems, Shape, shape processing, Three dimensional displays, Training},
	Month = nov,
	Pages = {1189--1195},
	Shorttitle = {Going into depth},
	Title = {Going into depth: {Evaluating} 2D and 3D cues for object classification on a new, large-scale object dataset},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCVW.2011.6130385}}

@inproceedings{song_sun_2015,
	Author = {Song, Shuran and Lichtenberg, Samuel P. and Xiao, Jianxiong},
	Date-Added = {2017-06-28 07:51:01 +0000},
	Date-Modified = {2017-06-28 07:51:01 +0000},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/7TDRDD4A/Song et al. - 2015 - SUN RGB-D A RGB-D Scene Understanding Benchmark S.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/REFCI5MN/Song_SUN_RGB-D_A_2015_CVPR_paper.html:text/html},
	Pages = {567--576},
	Shorttitle = {{SUN} {RGB}-{D}},
	Title = {{SUN} {RGB}-{D}: {A} {RGB}-{D} {Scene} {Understanding} {Benchmark} {Suite}},
	Url = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Song_SUN_RGB-D_A_2015_CVPR_paper.html},
	Urldate = {2017-06-28},
	Year = {2015},
	Bdsk-Url-1 = {http://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Song_SUN_RGB-D_A_2015_CVPR_paper.html}}

@inproceedings{silberman_indoor_2012,
	Abstract = {We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.},
	Author = {Silberman, Nathan and Hoiem, Derek and Kohli, Pushmeet and Fergus, Rob},
	Booktitle = {Computer {Vision} -- {ECCV} 2012},
	Date-Added = {2017-06-28 07:43:32 +0000},
	Date-Modified = {2017-06-28 07:43:32 +0000},
	Doi = {10.1007/978-3-642-33715-4_54},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/VCMHRCPV/10.html:text/html},
	Isbn = {978-3-642-33714-7 978-3-642-33715-4},
	Language = {en},
	Month = oct,
	Pages = {746--760},
	Publisher = {Springer, Berlin, Heidelberg},
	Series = {Lecture {Notes} in {Computer} {Science}},
	Title = {Indoor {Segmentation} and {Support} {Inference} from {RGBD} {Images}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54},
	Urldate = {2017-06-28},
	Year = {2012},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-642-33715-4_54},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-642-33715-4_54}}

@article{panicker_design_2016,
	Author = {Panicker, Sanjana and KV, Maitreyi and Gonsalves, Merrill and Gonsalves, Kane and Anand, Sandeep and Pati, Sandhya},
	Issn = {2321-7782},
	Journal = {International Journal of Advance Research in Computer Science and Management Studies},
	Month = oct,
	Number = {10},
	Pages = {90--98},
	Title = {Design and {Overview} of a {Navigation} {Application} for the {Blind}},
	Url = {http://www.ijarcsms.com/docs/paper/volume4/issue10/V4I10-0017.pdf},
	Volume = {4},
	Year = {2016},
	Bdsk-Url-1 = {http://www.ijarcsms.com/docs/paper/volume4/issue10/V4I10-0017.pdf}}

@article{guerrero_indoor_2012,
	Abstract = {Navigation in indoor environments is highly challenging for the severely visually impaired, particularly in spaces visited for the first time. Several solutions have been proposed to deal with this challenge. Although some of them have shown to be useful in real scenarios, they involve an important deployment effort or use artifacts that are not natural for blind users. This paper presents an indoor navigation system that was designed taking into consideration usability as the quality requirement to be maximized. This solution enables one to identify the position of a person and calculates the velocity and direction of his movements. Using this information, the system determines the user's trajectory, locates possible obstacles in that route, and offers navigation information to the user. The solution has been evaluated using two experimental scenarios. Although the results are still not enough to provide strong conclusions, they indicate that the system is suitable to guide visually impaired people through an unknown built environment.},
	Author = {Guerrero, Luis A. and Vasquez, Francisco and Ochoa, Sergio F.},
	Copyright = {http://creativecommons.org/licenses/by/3.0/},
	Doi = {10.3390/s120608236},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/N6NDPMBT/Guerrero et al. - 2012 - An Indoor Navigation System for the Visually Impai.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/N473J39S/htm.html:text/html},
	Journal = {Sensors},
	Keywords = {augmented object, indoor positioning, movement detection, navigation system, visually impaired, voice interface},
	Language = {en},
	Month = jun,
	Number = {6},
	Pages = {8236--8258},
	Title = {An {Indoor} {Navigation} {System} for the {Visually} {Impaired}},
	Url = {http://www.mdpi.com/1424-8220/12/6/8236},
	Urldate = {2017-06-13},
	Volume = {12},
	Year = {2012},
	Bdsk-Url-1 = {http://www.mdpi.com/1424-8220/12/6/8236},
	Bdsk-Url-2 = {http://dx.doi.org/10.3390/s120608236}}

@article{bradley_experimental_2005,
	Abstract = {In recent years, there has been an escalation of orientation and wayfinding technologies and systems for visually impaired people. These technological advancements, however, have not been matched by a suitable investigation of human-computer interaction (e.g. designing navigation aids for people who form different cognitive maps for navigation). The aim of this study is to investigate whether a group of sighted participants and a group of visually impaired participants experience a difference in mental and physical demands when given two different sets of verbal instructions directing them to four landmarks. The content of the first set of instructions was proportioned to route descriptions derived from sighted people, and the second set proportioned to descriptions derived from visually impaired people. The objective assessment involved measuring the time taken by participants to reach landmarks and the number of deviations that occurred. A NASA--Task Load Index questionnaire provided an indication of participants subjective perception of workload. The results revealed that instructions formed from visually impaired people resulted in a lower weighted workload score, less minor deviations, and quicker times for visually impaired participants. In contrast, these instructions were found to cause a higher weighted workload score for sighted participants. The results are discussed in relation to the issue of personalisation of mobile context-aware systems for visually impaired people.},
	Author = {Bradley, Nicholas A. and Dunlop, Mark D.},
	Doi = {10.1007/s00779-005-0350-y},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/ZHBXCXXQ/s00779-005-0350-y.html:text/html},
	Issn = {1617-4909, 1617-4917},
	Journal = {Pers Ubiquit Comput},
	Language = {en},
	Month = nov,
	Number = {6},
	Pages = {395--403},
	Title = {An {Experimental} {Investigation} into {Wayfinding} {Directions} for {Visually} {Impaired} {People}},
	Url = {https://link.springer.com/article/10.1007/s00779-005-0350-y},
	Urldate = {2017-06-13},
	Volume = {9},
	Year = {2005},
	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s00779-005-0350-y},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s00779-005-0350-y}}

@article{serrao_navigation_2014,
	Abstract = {In an unfamiliar environment we spot and explore all available information which might guide us to a desired location. This largely unconscious processing is done by our trained sensory and cognitive systems. These recognise and memorise sets of landmarks which allow us to create a mental map of the environment, and this map enables us to navigate by exploiting very few but the most important landmarks stored in our memory. In this paper we present a route planning, localisation and navigation system which works in real time. It integrates a geographic information system of a building with visual landmarks for localising the user and for validating the navigation route. Although designed for visually impaired persons, the system can also be employed to assist or transport persons with reduced mobility in way finding in a complex building.},
	Author = {Serr{\~a}o, M. and Rodrigues, J. M. F. and Buf, J. M. H. du},
	Doi = {10.1016/j.procs.2014.02.005},
	File = {ScienceDirect Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/KTV26T43/Serr{\~a}o et al. - 2014 - Navigation Framework Using Visual Landmarks and a .pdf:application/pdf;ScienceDirect Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/99DD6IDK/S1877050914000076.html:text/html},
	Issn = {1877-0509},
	Journal = {Procedia Computer Science},
	Keywords = {Accessibility, Geographic information system, Navigation, Vision, Visually impaired persons},
	Month = jan,
	Pages = {28--37},
	Series = {5th {International} {Conference} on {Software} {Development} and {Technologies} for {Enhancing} {Accessibility} and {Fighting} {Info}-exclusion, {DSAI} 2013},
	Title = {Navigation {Framework} {Using} {Visual} {Landmarks} and a {GIS}},
	Url = {http://www.sciencedirect.com/science/article/pii/S1877050914000076},
	Volume = {27},
	Year = {2014},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1877050914000076},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.procs.2014.02.005}}

@inproceedings{schmitz_acquisition_2011,
	Abstract = {In order to allow blind people independent navigation in unknown areas, we have developed a navigation system that seamlessly integrates both static maps and dynamic location-based textual information from a variety of sources. Each information source requires a different kind of acquisition technique. The acquired information is integrated by a context management platform and then presented to the user on a tactile-acoustical map depending on the sources available for his current position. Positioning is achieved by a combination of an inertial tracking system, RFID technology and GPS and the user is guided to a desired destination by speech output and a hap tic cane. The resulting system is the first of its kind to integrate a variety of maps and other accumulated location-based information on a unified interface for blind people.},
	Author = {Schmitz, B. and Becker, S. and Blessing, A. and Gro{\ss}mann, M.},
	Booktitle = {2011 {IEEE} 12th {International} {Conference} on {Mobile} {Data} {Management}},
	Doi = {10.1109/MDM.2011.66},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/V6SP4ZK8/6068447.html:text/html},
	Keywords = {acoustic signal processing, acquired information, acquisition technique, blind navigation, blind people independent navigation, Buildings, cartography, Context, context management platform, data acquisition, data presentation, diverse spatial context data, dynamic location-based textual information, Global Positioning System, GPS, handicapped aids, haptic cane, haptic interfaces, inertial tracking system, information source, location-based information, mobile computing, navigation system, RFID technology, Servers, spatial data structures, speech output, static maps, tactile-acoustical map, Three dimensional displays, unified interface, Vibrations},
	Month = jun,
	Pages = {276--284},
	Title = {Acquisition and {Presentation} of {Diverse} {Spatial} {Context} {Data} for {Blind} {Navigation}},
	Volume = {1},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/MDM.2011.66}}

@misc{noauthor_indooratlas_nodate,
	File = {IndoorAtlas pioneers its location technology with passion | Computer Science and Engineering:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/MUWPPDXM/17779.html:text/html},
	Title = {{IndoorAtlas} pioneers its location technology with passion {\textbar} {Computer} {Science} and {Engineering}},
	Url = {http://www.oulu.fi/cse/node/17779},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://www.oulu.fi/cse/node/17779}}

@misc{noauthor_seeing_nodate,
	Abstract = {Pivothead Wearable Imaging is a pioneer in smart eyewear and wearable cameras, and provides enterprise solutions around POV video, photo, live streaming, and video VoIP conferencing calls, all offered as separate applications on our visual communications IOT platform.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/SKUTKGMZ/seeingai.html:text/html},
	Journal = {Pivothead Wearable Imaging},
	Title = {Seeing {AI} {Project}},
	Url = {http://www.pivothead.com/seeingai/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://www.pivothead.com/seeingai/}}

@misc{noauthor_openstreetmap_nodate,
	Abstract = {OpenStreetMap is a map of the world, created by people like you and free to use under an open licence.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/FJ3KXVSU/www.openstreetmap.org.html:text/html},
	Journal = {OpenStreetMap},
	Title = {{OpenStreetMap}},
	Url = {https://www.openstreetmap.org/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://www.openstreetmap.org/}}

@misc{noauthor_google_nodate,
	Abstract = {Embed real-world imagery with 360-degree panoramas using the Google Street View Image API.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/4IBCBNXA/streetview.html:text/html},
	Journal = {Google Developers},
	Title = {Google {Street} {View} {Image} {API}},
	Url = {https://developers.google.com/maps/documentation/streetview/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://developers.google.com/maps/documentation/streetview/}}

@misc{noauthor_getting_nodate,
	Abstract = {Tango adds extra sensors to an Android device for computer vision and depth perception. It can learn its surroundings visually and understand its position in 3D space. The C API gives developers native access to the device's sensor information.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/Z9I6WZV6/c.html:text/html},
	Journal = {Google Developers},
	Title = {Getting {Started} with the {Tango} {C} {API} {\textbar} {Tango} {C} {API}},
	Url = {https://developers.google.com/tango/apis/c/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://developers.google.com/tango/apis/c/}}

@inproceedings{song_sliding_2014,
	Abstract = {The depth information of RGB-D sensors has greatly simplified some common challenges in computer vision and enabled breakthroughs for several tasks. In this paper, we propose to use depth maps for object detection and design a 3D detector to overcome the major difficulties for recognition, namely the variations of texture, illumination, shape, viewpoint, clutter, occlusion, self-occlusion and sensor noises. We take a collection of 3D CAD models and render each CAD model from hundreds of viewpoints to obtain synthetic depth maps. For each depth rendering, we extract features from the 3D point cloud and train an Exemplar-SVM classifier. During testing and hard-negative mining, we slide a 3D detection window in 3D space. Experiment results show that our 3D detector significantly outperforms the state-of-the-art algorithms for both RGB and RGB-D images, and achieves about ×1.7 improvement on average precision compared to DPM and R-CNN. All source code and data are available online.},
	Author = {Song, Shuran and Xiao, Jianxiong},
	Booktitle = {Computer {Vision} -- {ECCV} 2014},
	Doi = {10.1007/978-3-319-10599-4_41},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/TXNCW9K5/978-3-319-10599-4_41.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {634--651},
	Publisher = {Springer, Cham},
	Title = {Sliding {Shapes} for 3D {Object} {Detection} in {Depth} {Images}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_41},
	Urldate = {2017-06-06},
	Year = {2014},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_41},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-10599-4_41}}

@misc{marburg_opencv-ffi-ext:_2017,
	Author = {Marburg, Aaron},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/MGMRFM7N/opensurf.html:text/html},
	Month = mar,
	Note = {original-date: 2012-05-07T07:29:38Z},
	Shorttitle = {opencv-ffi-ext},
	Title = {opencv-ffi-ext: {Native} compiled extensions to {Opencv}-{FFI}},
	Url = {https://github.com/amarburg/opencv-ffi-ext},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/amarburg/opencv-ffi-ext}}

@article{lingua_performance_2009,
	Abstract = {In the photogrammetry field, interest in region detectors, which are widely used in Computer Vision, is quickly increasing due to the availability of new techniques. Images acquired by Mobile Mapping Technology, Oblique Photogrammetric Cameras or Unmanned Aerial Vehicles do not observe normal acquisition conditions. Feature extraction and matching techniques, which are traditionally used in photogrammetry, are usually inefficient for these applications as they are unable to provide reliable results under extreme geometrical conditions (convergent taking geometry, strong affine transformations, etc.) and for bad-textured images. A performance analysis of the SIFT technique in aerial and close-range photogrammetric applications is presented in this paper. The goal is to establish the suitability of the SIFT technique for automatic tie point extraction and approximate DSM (Digital Surface Model) generation. First, the performances of the SIFT operator have been compared with those provided by feature extraction and matching techniques used in photogrammetry. All these techniques have been implemented by the authors and validated on aerial and terrestrial images. Moreover, an auto-adaptive version of the SIFT operator has been developed, in order to improve the performances of the SIFT detector in relation to the texture of the images. The Auto-Adaptive SIFT operator (A2 SIFT) has been validated on several aerial images, with particular attention to large scale aerial images acquired using mini-UAV systems.},
	Author = {Lingua, Andrea and Marenchino, Davide and Nex, Francesco},
	Copyright = {http://creativecommons.org/licenses/by/3.0/},
	Doi = {10.3390/s90503745},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/D2S54BJG/Lingua et al. - 2009 - Performance Analysis of the SIFT Operator for Auto.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/5MJ3D9CA/3745.html:text/html},
	Journal = {Sensors},
	Keywords = {feature extraction, feature matching, image orientation, location accuracy, SIFT operator},
	Language = {en},
	Month = may,
	Number = {5},
	Pages = {3745--3766},
	Title = {Performance {Analysis} of the {SIFT} {Operator} for {Automatic} {Feature} {Extraction} and {Matching} in {Photogrammetric} {Applications}},
	Url = {http://www.mdpi.com/1424-8220/9/5/3745},
	Urldate = {2017-06-12},
	Volume = {9},
	Year = {2009},
	Bdsk-Url-1 = {http://www.mdpi.com/1424-8220/9/5/3745},
	Bdsk-Url-2 = {http://dx.doi.org/10.3390/s90503745}}

@inproceedings{jafri_obstacle_2016,
	Abstract = {A depth-data based obstacle detection and avoidance application for VI users to assist them in navigating independently in previously unmapped indoors environments is presented. The application is being developed for the recently introduced Google Project Tango Tablet Development Kit equipped with a powerful processor (NVIDIA Tegra K1 with 192 CUDA cores) as well as var-ious sensors which allow it track its motion and orientation in 3D space in real-time. Depth data for the area in front of the users, obtained using the tablet's in-built infrared--based depth sensor, is analyzed to detect obstacles and audio-based navigation instructions are provided accordingly. A visual display option is also offered for users with low vision. The aim is to develop a real-time, affordable, aesthetically acceptable, mobile assistive stand-alone application on a cutting-edge device, adopting a user-centered approach, which allows VI users to micro-navigate autonomously in possibly unfamiliar indoor surroundings.},
	Author = {Jafri, Rabia and Khan, Marwa Mahmoud},
	Booktitle = {Computers {Helping} {People} with {Special} {Needs}},
	Doi = {10.1007/978-3-319-41267-2_24},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/6S7A8GSS/978-3-319-41267-2_24.html:text/html},
	Language = {en},
	Month = jul,
	Pages = {179--185},
	Publisher = {Springer, Cham},
	Title = {Obstacle {Detection} and {Avoidance} for the {Visually} {Impaired} in {Indoors} {Environments} {Using} {Google}'s {Project} {Tango} {Device}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-41267-2_24},
	Urldate = {2017-06-06},
	Year = {2016},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-41267-2_24},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-41267-2_24}}

@book{sonka_image_2014,
	Abstract = {The brand new edition of IMAGE PROCESSING, ANALYSIS, AND MACHINE VISION is a robust text providing deep and wide coverage of the full range of topics encountered in the field of image processing and machine vision. As a result, it can serve undergraduates, graduates, researchers, and professionals looking for a readable reference. The book's encyclopedic coverage of topics is wide, and it can be used in more than one course (both image processing and machine vision classes). In addition, while advanced mathematics is not needed to understand basic concepts (making this a good choice for undergraduates), rigorous mathematical coverage is included for more advanced readers. It is also distinguished by its easy-to-understand algorithm descriptions of difficult concepts, and a wealth of carefully selected problems and examples.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
	Author = {Sonka, Milan and Hlavac, Vaclav and Boyle, Roger},
	Isbn = {978-1-285-98144-4},
	Keywords = {Technology \& Engineering / Engineering (General)},
	Language = {en},
	Month = jan,
	Note = {Google-Books-ID: QePKAgAAQBAJ},
	Publisher = {Cengage Learning},
	Title = {Image {Processing}, {Analysis}, and {Machine} {Vision}},
	Year = {2014}}

@misc{evans_opensurf_2015,
	Author = {Evans, Chris},
	File = {OpenSURF - The Official Home of the Image Processing Library:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/7DX3KKTZ/computer-vision-opensurf.html:text/html},
	Month = feb,
	Title = {{OpenSURF} - {The} {Official} {Home} of the {Image} {Processing} {Library}},
	Url = {https://web.archive.org/web/20150225125650/http://www.chrisevansdev.com/computer-vision-opensurf.html},
	Year = {2015},
	Bdsk-Url-1 = {https://web.archive.org/web/20150225125650/http://www.chrisevansdev.com/computer-vision-opensurf.html}}

@misc{noauthor_visualsfm_nodate,
	File = {VisualSFM \: A Visual Structure from Motion System:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/PWPR9XJQ/vsfm.html:text/html},
	Title = {{VisualSFM} : {A} {Visual} {Structure} from {Motion} {System}},
	Url = {http://ccwu.me/vsfm/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://ccwu.me/vsfm/}}

@misc{noauthor_pmvs2_nodate,
	File = {PMVS2:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3WAEK42U/pmvs.html:text/html},
	Title = {{PMVS}2},
	Url = {http://www.di.ens.fr/pmvs/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://www.di.ens.fr/pmvs/}}

@misc{noauthor_cydalion._nodate,
	File = {Cydalion. Navigation App for the Blind and Visually Impaired:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/F2ZV58DB/cydalion.com.html:text/html},
	Title = {Cydalion. {Navigation} {App} for the {Blind} and {Visually} {Impaired}},
	Url = {http://cydalion.com/#features},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://cydalion.com/#features}}

@misc{author_ariadne_nodate,
	Abstract = {There are hundreds of gps apps},
	Author = {Author, AppAdvice Staff},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/IMQVJMN8/441063072.html:text/html},
	Journal = {AppAdvice},
	Title = {Ariadne {GPS} by {Giovanni} {Ciaffoni}},
	Url = {http://www.ariadnegps.eu/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {/app/ariadne-gps/441063072}}

@misc{noauthor_ariane_nodate,
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C67QG4N2/how.html:text/html},
	Journal = {Mability},
	Title = {Ariane {Project}},
	Url = {http://arianeproject.com/how/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {http://arianeproject.com/how/}}

@misc{noauthor_tango_nodate,
	Abstract = {See more of your world.},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/6P5Q2S2W/tango.html:text/html},
	Journal = {Tango},
	Title = {Tango},
	Url = {https://get.google.com/tango/},
	Urldate = {2017-06-13},
	Bdsk-Url-1 = {https://get.google.com/tango/}}

@inproceedings{riehle_indoor_2008,
	Abstract = {Indoor navigation technology is needed to support seamless mobility for the visually impaired. A small portable personal navigation device that provides current position, useful contextual wayfinding information about the indoor environment and directions to a destination would greatly improve access and independence for people with low vision. This paper describes the construction of such a device which utilizes a commercial Ultra-Wideband (UWB) asset tracking system to support real-time location and navigation information. Human trials were conducted to assess the efficacy of the system by comparing target-finding performance between blindfolded subjects using the navigation system for real-time guidance, and blindfolded subjects who only received speech information about their local surrounds but no route guidance information (similar to that available from a long cane or guide dog). A normal vision control condition was also run. The time and distance traveled was measured in each trial and a point-back test was performed after goal completion to assess cognitive map development. Statistically significant differences were observed between the three conditions in time and distance traveled; with the navigation system and the visual condition yielding the best results, and the navigation system dramatically outperforming the non-guided condition.},
	Author = {Riehle, T. H. and Lichter, P. and Giudice, N. A.},
	Booktitle = {2008 30th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society}},
	Doi = {10.1109/IEMBS.2008.4650195},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/35HIKIE7/4650195.html:text/html},
	Keywords = {Algorithms, Biomedical Engineering, Blindness, Canes, Equipment Design, Humans, Indoor environments, Microcomputers, Navigation, Performance evaluation, Real time systems, Sensory Aids, Software, Speech, Target tracking, Testing, Time measurement, Ultra wideband technology, User-Computer Interface, Visual Fields, Visually impaired persons, Walking},
	Month = aug,
	Pages = {4435--4438},
	Title = {An indoor navigation system to support the visually impaired},
	Year = {2008},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/IEMBS.2008.4650195}}

@inproceedings{quinones_supporting_2011,
	Abstract = {In this paper, we investigate the requirements for designing systems to support wayfinding for visually impaired individuals. We report the results of an interview study with 20 individuals with visual impairments, asking about their way-finding tools, techniques, and obstacles. Our findings provide an account of the practices followed when navigating familiar, unfamiliar, and dynamic environments, and common breakdowns encountered during the wayfinding process. The findings from this study suggest ways of implementing a location-based system to assist in the recovery from various obstacles.},
	Address = {New York, NY, USA},
	Author = {Quinones, Pablo-Alejandro and Greene, Tammy and Yang, Rayoung and Newman, Mark},
	Booktitle = {{CHI} '11 {Extended} {Abstracts} on {Human} {Factors} in {Computing} {Systems}},
	Doi = {10.1145/1979742.1979822},
	Isbn = {978-1-4503-0268-5},
	Keywords = {Accessibility, location-based computing, visual impairment, wayfinding},
	Pages = {1645--1650},
	Publisher = {ACM},
	Series = {{CHI} {EA} '11},
	Shorttitle = {Supporting {Visually} {Impaired} {Navigation}},
	Title = {Supporting {Visually} {Impaired} {Navigation}: {A} {Needs}-finding {Study}},
	Url = {http://doi.acm.org/10.1145/1979742.1979822},
	Year = {2011},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1979742.1979822},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1979742.1979822}}

@inproceedings{zollner_navi_2011,
	Abstract = {We present a proof-of-concept of a mobile navigational aid that uses the Microsoft Kinect and optical marker tracking to help visually impaired people find their way inside buildings. The system is the result of a student project and is entirely based on low-cost hard- and software. It provides continuous vibrotactile feedback on the person's waist, to give an impression of the environment and to warn about obstacles. Furthermore, optical markers can be used to tag points of interest within the building to enable synthesized voice instructions for point-to-point navigation.},
	Author = {Z{\"o}llner, Michael and Huber, Stephan and Jetter, Hans-Christian and Reiterer, Harald},
	Booktitle = {Human-{Computer} {Interaction} -- {INTERACT} 2011},
	Doi = {10.1007/978-3-642-23768-3_88},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/GT4Q48VQ/Z{\"o}llner et al. - 2011 - NAVI -- A Proof-of-Concept of a Mobile Navigational.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/JKS8HB44/10.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {584--587},
	Publisher = {Springer, Berlin, Heidelberg},
	Title = {{NAVI} -- {A} {Proof}-of-{Concept} of a {Mobile} {Navigational} {Aid} for {Visually} {Impaired} {Based} on the {Microsoft} {Kinect}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-642-23768-3_88},
	Urldate = {2017-06-13},
	Year = {2011},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-642-23768-3_88},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-642-23768-3_88}}

@article{chapelle_support_1999,
	Abstract = {Traditional classification approaches generalize poorly on image classification tasks, because of the high dimensionality of the feature space. This paper shows that support vector machines (SVM) can generalize well on difficult image classification problems where the only features are high dimensional histograms. Heavy-tailed RBF kernels of the form K(x, y)=e-ρΣi{\textbar}xia-yia{\textbar}b with a ⩽1 and b⩽2 are evaluated on the classification of images extracted from the Corel stock photo collection and shown to far outperform traditional polynomial or Gaussian radial basis function (RBF) kernels. Moreover, we observed that a simple remapping of the input xi→xia improves the performance of linear SVM to such an extend that it makes them, for this problem, a valid alternative to RBF kernels},
	Author = {Chapelle, O. and Haffner, P. and Vapnik, V. N.},
	Doi = {10.1109/72.788646},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C93BAXGJ/788646.html:text/html},
	Issn = {1045-9227},
	Journal = {IEEE Transactions on Neural Networks},
	Keywords = {Classification tree analysis, Corel stock photo collection, feature space dimensionality, heavy-tailed RBF kernels, high-dimensional histograms, histogram-based image classification, Histograms, image classification, Image databases, Image recognition, Kernel, learning (artificial intelligence), linear SVM, Polynomials, radial basis function networks, remapping, Support vector machine classification, support vector machines, Web pages},
	Month = sep,
	Number = {5},
	Pages = {1055--1064},
	Title = {Support vector machines for histogram-based image classification},
	Volume = {10},
	Year = {1999},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/72.788646}}

@inproceedings{ciregan_multi-column_2012,
	Abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
	Author = {Ciregan, D. and Meier, U. and Schmidhuber, J.},
	Booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	Doi = {10.1109/CVPR.2012.6248110},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C7UP7AGI/6248110.html:text/html},
	Keywords = {artificial neural network architectures, Benchmark testing, Computer architecture, computer vision, convolutional winner-take-all neurons, Error analysis, fast training, graphics cards, Graphics processing unit, graphics processing units, handwritten character recognition, handwritten digits recognition, human performance, image classification, Image recognition, learning (artificial intelligence), machine learning, MNIST handwriting benchmark, multicolumn deep neural networks, neural nets, Neurons, retina, sparsely connected neural layers, traffic sign recognition benchmark, traffic signs, Training, visual cortex},
	Month = jun,
	Pages = {3642--3649},
	Title = {Multi-column deep neural networks for image classification},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2012.6248110}}

@article{duro_comparison_2012,
	Abstract = {Pixel-based and object-based image analysis approaches for classifying broad land cover classes over agricultural landscapes are compared using three supervised machine learning algorithms: decision tree (DT), random forest (RF), and the support vector machine (SVM). Overall classification accuracies between pixel-based and object-based classifications were not statistically significant (p{\textgreater}0.05) when the same machine learning algorithms were applied. Using object-based image analysis, there was a statistically significant difference in classification accuracy between maps produced using the DT algorithm compared to maps produced using either RF (p=0.0116) or SVM algorithms (p=0.0067). Using pixel-based image analysis, there was no statistically significant difference (p{\textgreater}0.05) between results produced using different classification algorithms. Classifications based on RF and SVM algorithms provided a more visually adequate depiction of wetland, riparian, and crop land cover types when compared to DT based classifications, using either object-based or pixel-based image analysis. In this study, pixel-based classifications utilized fewer variables (15 vs. 300), achieved similar classification accuracies, and required less time to produce than object-based classifications. Object-based classifications produced a visually appealing generalized appearance of land cover classes. Based exclusively on overall accuracy reports, there was no advantage to preferring one image analysis approach over another for the purposes of mapping broad land cover types in agricultural environments using medium spatial resolution earth observation imagery.},
	Author = {Duro, Dennis C. and Franklin, Steven E. and Dub{\'e}, Monique G.},
	Doi = {10.1016/j.rse.2011.11.020},
	File = {ScienceDirect Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/NR23EKKS/S0034425711004172.html:text/html},
	Issn = {0034-4257},
	Journal = {Remote Sensing of Environment},
	Keywords = {Comparison, Decision tree, Object-based, Random forest, Support vector machine},
	Month = mar,
	Pages = {259--272},
	Title = {A comparison of pixel-based and object-based image analysis with selected machine learning algorithms for the classification of agricultural landscapes using {SPOT}-5 {HRG} imagery},
	Url = {http://www.sciencedirect.com/science/article/pii/S0034425711004172},
	Volume = {118},
	Year = {2012},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0034425711004172},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.rse.2011.11.020}}

@misc{noauthor_list_nodate,
	File = {List of OSM-based services - OpenStreetMap Wiki:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/5SEXGCK2/List_of_OSM-based_services.html:text/html},
	Title = {List of {OSM}-based services - {OpenStreetMap} {Wiki}},
	Url = {http://wiki.openstreetmap.org/wiki/List_of_OSM-based_services#Routing},
	Urldate = {2017-06-20},
	Bdsk-Url-1 = {http://wiki.openstreetmap.org/wiki/List_of_OSM-based_services#Routing}}

@inproceedings{knopp_hough_2010,
	Abstract = {Most methods for the recognition of shape classes from 3D datasets focus on classifying clean, often manually generated models. However, 3D shapes obtained through acquisition techniques such as Structure-from-Motion or LIDAR scanning are noisy, clutter and holes. In that case global shape features---still dominating the 3D shape class recognition literature---are less appropriate. Inspired by 2D methods, recently researchers have started to work with local features. In keeping with this strand, we propose a new robust 3D shape classification method. It contains two main contributions. First, we extend a robust 2D feature descriptor, SURF, to be used in the context of 3D shapes. Second, we show how 3D shape class recognition can be improved by probabilistic Hough transform based methods, already popular in 2D. Through our experiments on partial shape retrieval, we show the power of the proposed 3D features. Their combination with the Hough transform yields superior results for class recognition on standard datasets. The potential for the applicability of such a method in classifying 3D obtained from Structure-from-Motion methods is promising, as we show in some initial experiments.},
	Author = {Knopp, Jan and Prasad, Mukta and Willems, Geert and Timofte, Radu and Gool, Luc Van},
	Booktitle = {Computer {Vision} -- {ECCV} 2010},
	Doi = {10.1007/978-3-642-15567-3_43},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/7KJ9W2JK/Knopp et al. - 2010 - Hough Transform and 3D SURF for Robust Three Dimen.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3HBJTWK9/10.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {589--602},
	Publisher = {Springer, Berlin, Heidelberg},
	Title = {Hough {Transform} and 3D {SURF} for {Robust} {Three} {Dimensional} {Classification}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-642-15567-3_43},
	Urldate = {2017-06-21},
	Year = {2010},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-642-15567-3_43},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-642-15567-3_43}}

@inproceedings{ren_rgb-d_2012,
	Abstract = {Scene labeling research has mostly focused on outdoor scenes, leaving the harder case of indoor scenes poorly understood. Microsoft Kinect dramatically changed the landscape, showing great potentials for RGB-D perception (color+depth). Our main objective is to empirically understand the promises and challenges of scene labeling with RGB-D. We use the NYU Depth Dataset as collected and analyzed by Silberman and Fergus [30]. For RGB-D features, we adapt the framework of kernel descriptors that converts local similarities (kernels) to patch descriptors. For contextual modeling, we combine two lines of approaches, one using a superpixel MRF, and the other using a segmentation tree. We find that (1) kernel descriptors are very effective in capturing appearance (RGB) and shape (D) similarities; (2) both superpixel MRF and segmentation tree are useful in modeling context; and (3) the key to labeling accuracy is the ability to efficiently train and test with large-scale data. We improve labeling accuracy on the NYU Dataset from 56.6\% to 76.1\%. We also apply our approach to image-only scene labeling and improve the accuracy on the Stanford Background Dataset from 79.4\% to 82.9\%.},
	Author = {Ren, X. and Bo, L. and Fox, D.},
	Booktitle = {2012 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	Doi = {10.1109/CVPR.2012.6247999},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/TTDVTV58/6247999.html:text/html},
	Keywords = {Accuracy, Context modeling, contextual modeling, Image color analysis, image colour analysis, image segmentation, indoor scenes, Kernel, kernel descriptors, Labeling, local similarities, Microsoft Kinect, outdoor scenes, patch descriptors, RGB-D features, RGB-D perception, RGB-(D) scene labeling, scene labeling research, segmentation tree, superpixel MRF, trees (mathematics), Vegetation},
	Month = jun,
	Pages = {2759--2766},
	Shorttitle = {{RGB}-({D}) scene labeling},
	Title = {{RGB}-({D}) scene labeling: {Features} and algorithms},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CVPR.2012.6247999}}

@incollection{bo_unsupervised_2013,
	Abstract = {Recently introduced RGB-D cameras are capable of providing high quality synchronized videos of both color and depth. With its advanced sensing capabilities, this technology represents an opportunity to dramatically increase the capabilities of object recognition. It also raises the problem of developing expressive features for the color and depth channels of these sensors. In this paper we introduce hierarchical matching pursuit (HMP) for RGB-D data. HMP uses sparse coding to learn hierarchical feature representations from raw RGB-D data in an unsupervised way. Extensive experiments on various datasets indicate that the features learned with our approach enable superior object recognition results using linear support vector machines.},
	Author = {Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
	Booktitle = {Experimental {Robotics}},
	Copyright = {{\copyright}2013 Springer International Publishing Switzerland},
	Editor = {Desai, Jaydev P. and Dudek, Gregory and Khatib, Oussama and Kumar, Vijay},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/FDWMMV8B/Bo et al. - 2013 - Unsupervised Feature Learning for RGB-D Based Obje.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/XJVGIAZ6/978-3-319-00065-7_27.html:text/html},
	Isbn = {978-3-319-00064-0 978-3-319-00065-7},
	Keywords = {Artificial Intelligence (incl. Robotics), Robotics and Automation},
	Language = {en},
	Note = {DOI: 10.1007/978-3-319-00065-7\_27},
	Number = {88},
	Pages = {387--402},
	Publisher = {Springer International Publishing},
	Series = {Springer {Tracts} in {Advanced} {Robotics}},
	Title = {Unsupervised {Feature} {Learning} for {RGB}-{D} {Based} {Object} {Recognition}},
	Url = {http://link.springer.com/chapter/10.1007/978-3-319-00065-7_27},
	Urldate = {2017-06-21},
	Year = {2013},
	Bdsk-Url-1 = {http://link.springer.com/chapter/10.1007/978-3-319-00065-7_27}}

@inproceedings{lai_large-scale_2011,
	Abstract = {Over the last decade, the availability of public image repositories and recognition benchmarks has enabled rapid progress in visual object category and instance detection. Today we are witnessing the birth of a new generation of sensing technologies capable of providing high quality synchronized videos of both color and depth, the RGB-D (Kinect-style) camera. With its advanced sensing capabilities and the potential for mass adoption, this technology represents an opportunity to dramatically increase robotic object recognition, manipulation, navigation, and interaction capabilities. In this paper, we introduce a large-scale, hierarchical multi-view object dataset collected using an RGB-D camera. The dataset contains 300 objects organized into 51 categories and has been made publicly available to the research community so as to enable rapid progress based on this promising technology. This paper describes the dataset collection procedure and introduces techniques for RGB-D based object recognition and detection, demonstrating that combining color and depth information substantially improves quality of results.},
	Author = {Lai, K. and Bo, L. and Ren, X. and Fox, D.},
	Booktitle = {2011 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	Doi = {10.1109/ICRA.2011.5980382},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/ZS7APIKW/5980382.html:text/html},
	Keywords = {Cameras, dataset collection procedure, image colour analysis, image sensors, instance detection, large-scale hierarchical multiview RGB-D object dataset, object recognition, public image recognition, public image repositories, RGB-D camera, robotic object recognition, Robot sensing systems, robot vision, Three dimensional displays, Videos, Video sequences, video signal processing, Visualization, visual object category},
	Month = may,
	Pages = {1817--1824},
	Title = {A large-scale hierarchical multi-view {RGB}-{D} object dataset},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICRA.2011.5980382}}

@inproceedings{gupta_learning_2014,
	Abstract = {In this paper we study the problem of object detection for RGB-D images using semantically rich image and depth features. We propose a new geocentric embedding for depth images that encodes height above ground and angle with gravity for each pixel in addition to the horizontal disparity. We demonstrate that this geocentric embedding works better than using raw depth images for learning feature representations with convolutional neural networks. Our final object detection system achieves an average precision of 37.3\%, which is a 56\% relative improvement over existing methods. We then focus on the task of instance segmentation where we label pixels belonging to object instances found by our detector. For this task, we propose a decision forest approach that classifies pixels in the detection window as foreground or background using a family of unary and binary tests that query shape and geocentric pose features. Finally, we use the output from our object detectors in an existing superpixel classification framework for semantic scene segmentation and achieve a 24\% relative improvement over current state-of-the-art for the object categories that we study. We believe advances such as those represented in this paper will facilitate the use of perception in fields like robotics.},
	Author = {Gupta, Saurabh and Girshick, Ross and Arbel{\'a}ez, Pablo and Malik, Jitendra},
	Booktitle = {Computer {Vision} -- {ECCV} 2014},
	Doi = {10.1007/978-3-319-10584-0_23},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/E7R4DCJ7/Gupta et al. - 2014 - Learning Rich Features from RGB-D Images for Objec.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/J5GSJPNH/978-3-319-10584-0_23.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {345--360},
	Publisher = {Springer, Cham},
	Title = {Learning {Rich} {Features} from {RGB}-{D} {Images} for {Object} {Detection} and {Segmentation}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-10584-0_23},
	Urldate = {2017-06-21},
	Year = {2014},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-10584-0_23},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-10584-0_23}}

@article{shao_performance_2017,
	Abstract = {Deep Neural Networks for image/video classification have obtained much success in various computer vision applications. Existing deep learning algorithms are widely used on RGB images or video data. Meanwhile, with the development of low-cost RGB-D sensors (such as Microsoft Kinect and Xtion Pro-Live), high-quality RGB-D data can be easily acquired and used to enhance computer vision algorithms [14]. It would be interesting to investigate how deep learning can be employed for extracting and fusing features from RGB-D data. In this paper, after briefly reviewing the basic concepts of RGB-D information and four prevalent deep learning models (i.e., Deep Belief Networks (DBNs), Stacked Denoising Auto-Encoders (SDAE), Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) Neural Networks), we conduct extensive experiments on five popular RGB-D datasets including three image datasets and two video datasets. We then present a detailed analysis about the comparison between the learned feature representations from the four deep learning models. In addition, a few suggestions on how to adjust hyper-parameters for learning deep neural networks are made in this paper. According to the extensive experimental results, we believe that this evaluation will provide insights and a deeper understanding of different deep learning algorithms for RGB-D feature extraction and fusion.},
	Author = {Shao, Ling and Cai, Ziyun and Liu, Li and Lu, Ke},
	Doi = {10.1016/j.ins.2017.01.013},
	File = {ScienceDirect Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/M739N66K/Shao et al. - 2017 - Performance evaluation of deep feature learning fo.pdf:application/pdf;ScienceDirect Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/UGF4BD34/S0020025517300191.html:text/html},
	Issn = {0020-0255},
	Journal = {Information Sciences},
	Keywords = {Deep neural networks, Feature learning, Performance evaluation, RGB-D data},
	Month = apr,
	Pages = {266--283},
	Title = {Performance evaluation of deep feature learning for {RGB}-{D} image/video classification},
	Url = {http://www.sciencedirect.com/science/article/pii/S0020025517300191},
	Volume = {385--386},
	Year = {2017},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S0020025517300191},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.ins.2017.01.013}}

@incollection{aguado_supervised_2017,
	Abstract = {A system has been developed to detect postures and movements of people, using the skeleton information provided by the OpenNI library. A supervised learning approach has been used for generating static posture classifier models. In the case of movements, the focus has been done in clustering techniques. These models are included as part of the system software once generated, which reacts to postures and gestures made by any user. The automatic detection of postures is interesting for many applications, such as medical applications or intelligent interaction based on computer vision.},
	Author = {Aguado, A. and Rodr{\'\i}guez, I. and Lazkano, E. and Sierra, B.},
	Booktitle = {Converging {Clinical} and {Engineering} {Research} on {Neurorehabilitation} {II}},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/P7UK9X34/Aguado et al. - 2017 - Supervised + Unsupervised Classification for Human.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/CSNK5BXG/978-3-319-46669-9_130.html:text/html},
	Language = {en},
	Note = {DOI: 10.1007/978-3-319-46669-9\_130},
	Pages = {795--800},
	Publisher = {Springer, Cham},
	Shorttitle = {Supervised + {Unsupervised} {Classification} for {Human} {Pose} {Estimation} with {RGB}-{D} {Images}},
	Title = {Supervised + {Unsupervised} {Classification} for {Human} {Pose} {Estimation} with {RGB}-{D} {Images}: {A} {First} {Step} {Towards} a {Rehabilitation} {System}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-46669-9_130},
	Urldate = {2017-06-21},
	Year = {2017},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-46669-9_130}}

@inproceedings{song_deep_2016,
	Author = {Song, Shuran and Xiao, Jianxiong},
	Booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
	Date-Modified = {2017-06-29 09:04:51 +0000},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/ZU4R3F6F/Song and Xiao - 2016 - Deep Sliding Shapes for Amodal 3D Object Detection.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/AMIPDHA9/Song_Deep_Sliding_Shapes_CVPR_2016_paper.html:text/html},
	Title = {Deep {Sliding} {Shapes} for {Amodal} 3D {Object} {Detection} in {RGB}-{D} {Images}},
	Url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Sliding_Shapes_CVPR_2016_paper.html},
	Urldate = {2017-06-21},
	Year = {2016},
	Bdsk-Url-1 = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Song_Deep_Sliding_Shapes_CVPR_2016_paper.html}}

@article{sun_weakly-supervised_2017,
	Abstract = {This paper addresses the problem of RGBD object recognition in real-world applications, where large amounts of annotated training data are typically unavailable. To overcome this problem, we propose a novel, weakly-supervised learning architecture (DCNN-GPC) which combines parametric models (a pair of Deep Convolutional Neural Networks (DCNN) for RGB and D modalities) with non-parametric models (Gaussian Process Classification). Our system is initially trained using a small amount of labeled data, and then automatically prop- agates labels to large-scale unlabeled data. We first run 3D- based objectness detection on RGBD videos to acquire many unlabeled object proposals, and then employ DCNN-GPC to label them. As a result, our multi-modal DCNN can be trained end-to-end using only a small amount of human annotation. Finally, our 3D-based objectness detection and multi-modal DCNN are integrated into a real-time detection and recognition pipeline. In our approach, bounding-box annotations are not required and boundary-aware detection is achieved. We also propose a novel way to pretrain a DCNN for the depth modality, by training on virtual depth images projected from CAD models. We pretrain our multi-modal DCNN on public 3D datasets, achieving performance comparable to state-of-the-art methods on Washington RGBS Dataset. We then finetune the network by further training on a small amount of annotated data from our novel dataset of industrial objects (nuclear waste simulants). Our weakly supervised approach has demonstrated to be highly effective in solving a novel RGBD object recognition application which lacks of human annotations.},
	Annote = {Comment: 8 pages, 5 figures, submitted to conference},
	Author = {Sun, Li and Zhao, Cheng and Stolkin, Rustam},
	File = {arXiv\:1703.06370 PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/JHDNQ6AA/Sun et al. - 2017 - Weakly-supervised DCNN for RGB-D Object Recognitio.pdf:application/pdf;arXiv.org Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/KBWBS3NR/1703.html:text/html},
	Journal = {arXiv:1703.06370 [cs]},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Month = mar,
	Note = {arXiv: 1703.06370},
	Title = {Weakly-supervised {DCNN} for {RGB}-{D} {Object} {Recognition} in {Real}-{World} {Applications} {Which} {Lack} {Large}-scale {Annotated} {Training} {Data}},
	Url = {http://arxiv.org/abs/1703.06370},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.06370}}

@article{jafri_gpu-accelerated_2017,
	Abstract = {An application for the recently introduced Google Project Tango Tablet Development Kit to assist visually impaired (VI) users in understanding their environmental context by identifying and locating multiple faces and objects in their vicinity in real-time is presented. CUDA-based GPU-accelerated algorithms would be utilized to detect and recognize faces and objects from the visual data, while the locations of these entities relative to the user would be estimated from the depth data acquired via the tablet. The interaction would be speech based with the user being offered several options for requesting information about the identities and/or relative locations of face and objects. The aim is to create a portable, affordable, power-efficient, standalone assistive application to increase the autonomy of VI users which can run in real time on the device itself.},
	Author = {Jafri, Rabia},
	Doi = {10.1007/s11227-016-1891-8},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/SDGTXWXJ/Jafri - 2017 - A GPU-accelerated real-time contextual awareness a.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/I9XBGIET/s11227-016-1891-8.html:text/html},
	Issn = {0920-8542, 1573-0484},
	Journal = {J Supercomput},
	Language = {en},
	Month = feb,
	Number = {2},
	Pages = {887--899},
	Title = {A {GPU}-accelerated real-time contextual awareness application for the visually impaired on {Google}'s project {Tango} device},
	Url = {https://link.springer.com/article/10.1007/s11227-016-1891-8},
	Urldate = {2017-06-23},
	Volume = {73},
	Year = {2017},
	Bdsk-Url-1 = {https://link.springer.com/article/10.1007/s11227-016-1891-8},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/s11227-016-1891-8}}

@inproceedings{endres_evaluation_2012,
	Abstract = {We present an approach to simultaneous localization and mapping (SLAM) for RGB-D cameras like the Microsoft Kinect. Our system concurrently estimates the trajectory of a hand-held Kinect and generates a dense 3D model of the environment. We present the key features of our approach and evaluate its performance thoroughly on a recently published dataset, including a large set of sequences of different scenes with varying camera speeds and illumination conditions. In particular, we evaluate the accuracy, robustness, and processing time for three different feature descriptors (SIFT, SURF, and ORB). The experiments demonstrate that our system can robustly deal with difficult data in common indoor scenarios while being fast enough for online operation. Our system is fully available as open-source.},
	Author = {Endres, F. and Hess, J. and Engelhard, N. and Sturm, J. and Cremers, D. and Burgard, W.},
	Booktitle = {2012 {IEEE} {International} {Conference} on {Robotics} and {Automation}},
	Doi = {10.1109/ICRA.2012.6225199},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/PVGI76XG/6225199.html:text/html},
	Keywords = {Accuracy, Cameras, camera speed, dense 3D model, feature extraction, hand-held Kinect, illumination condition, Microsoft Kinect, open source, Optimization, RGB-D cameras, RGB-D SLAM system, robot vision, Runtime, simultaneous localization and mapping, SLAM (robots), Trajectory},
	Month = may,
	Pages = {1691--1696},
	Title = {An evaluation of the {RGB}-{D} {SLAM} system},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICRA.2012.6225199}}

@inproceedings{jafri_gps-based_2014,
	Abstract = {A GPS-based smartphone application for blind users is proposed which will allow them to record pedestrian routes to frequently visited destinations (e.g., supermarket, neighborhood mosque, etc.) and to retrieve them later for autonomous navigation. Unlike similar systems, which simply provide a route to a specified destination based on existing GPS maps which may not contain detailed information especially about pedestrian paths and alleys, our software is unique in that it will allow users to record a customized path to a particular destination based on personal considerations such as whether the area surrounding the route is well-lit and well-populated, the unevenness of the terrain and the absence of hazards (such as traffic intersections). A distress call option and auditory cues about user-specified obstacles will also be provided. The objective is to develop a low-cost, portable solution based on easily accessible technology to assist blind users in their daily outdoor mobility tasks.},
	Author = {Jafri, Rabia and Ali, Syed Abid},
	Booktitle = {{HCI} {International} 2014 - {Posters}' {Extended} {Abstracts}},
	Doi = {10.1007/978-3-319-07854-0_41},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/C5ZZ4FG8/Jafri and Ali - 2014 - A GPS-Based Personalized Pedestrian Route Recordin.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/2SSH7PAW/10.html:text/html},
	Language = {en},
	Month = jun,
	Pages = {232--237},
	Publisher = {Springer, Cham},
	Title = {A {GPS}-{Based} {Personalized} {Pedestrian} {Route} {Recording} {Smartphone} {Application} for the {Blind}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-07854-0_41},
	Urldate = {2017-06-23},
	Year = {2014},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-07854-0_41},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-07854-0_41}}

@misc{noauthor_stated_nodate,
	File = {Stated Preferences for Components of a Personal Guidance System for Nonvisual Navigation --- JVIB Abstract - American Foundation for the Blind:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/ZZT25E5N/newjvibabstract.html:text/html},
	Title = {Stated {Preferences} for {Components} of a {Personal} {Guidance} {System} for {Nonvisual} {Navigation} --- {JVIB} {Abstract} - {American} {Foundation} for the {Blind}},
	Url = {http://www.afb.org/jvib/newjvibabstract.asp?articleid=JVIB980303},
	Urldate = {2017-06-23},
	Bdsk-Url-1 = {http://www.afb.org/jvib/newjvibabstract.asp?articleid=JVIB980303}}

@inproceedings{koiner_gps_2012,
	Abstract = {In this paper, we present a software application that aims at translating data collected by a cell phone into voice signals to help the blind and visually impaired users navigate their way from a starting point to a destination point. This means that cell phone issues voice messages to guide the user along a predetermined path. The application utilizes existing hardware in cell phone devices, such as GPS, compass, etc. This project is designed to research and develop a framework for assistive technologies using Nokia mobile platform. The outcome is a software product, an application on Nokia cell phones, and the project has direct social benefits because it aids people who are visually impaired. An application was developed to show a proof of-concept for personal trip navigation using a cell phone (Nokia-N8). Similar applications exist for driving, but this application attempts to offer a similar goal to all people's trips.},
	Author = {Koiner, K. and Elmiligi, H. and Gebali, F.},
	Booktitle = {2012 {Seventh} {International} {Conference} on {Broadband}, {Wireless} {Computing}, {Communication} and {Applications}},
	Doi = {10.1109/BWCCA.2012.71},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/N9XIJEIJ/6363087.html:text/html},
	Keywords = {Accuracy, assistive technology, Azimuth, blind users, cell phone device, Compass, data acquisition, Databases, data collection, Global Positioning System, GPS, GPS waypoint application, handicapped aids, Magnetic resonance imaging, Magnetometers, mobile computing, Navigation, Nokia cell phone, Nokia mobile platform, Nokia N8, public domain software, smart phones, social benefits, software application, software product, trip navigation, visually impaired user, visually impaired users, voice mail, voice message, voice signal},
	Month = nov,
	Pages = {397--401},
	Title = {{GPS} {Waypoint} {Application}},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/BWCCA.2012.71}}

@inproceedings{malisiewicz_ensemble_2011,
	Abstract = {This paper proposes a conceptually simple but surprisingly powerful method which combines the effectiveness of a discriminative object detector with the explicit correspondence offered by a nearest-neighbor approach. The method is based on training a separate linear SVM classifier for every exemplar in the training set. Each of these Exemplar-SVMs is thus defined by a single positive instance and millions of negatives. While each detector is quite specific to its exemplar, we empirically observe that an ensemble of such Exemplar-SVMs offers surprisingly good generalization. Our performance on the PASCAL VOC detection task is on par with the much more complex latent part-based model of Felzenszwalb et al., at only a modest computational cost increase. But the central benefit of our approach is that it creates an explicit association between each detection and a single training exemplar. Because most detections show good alignment to their associated exemplar, it is possible to transfer any available exemplar meta-data (segmentation, geometric structure, 3D model, etc.) directly onto the detections, which can then be used as part of overall scene understanding.},
	Author = {Malisiewicz, T. and Gupta, A. and Efros, A. A.},
	Booktitle = {2011 {International} {Conference} on {Computer} {Vision}},
	Doi = {10.1109/ICCV.2011.6126229},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/NC8CWQ3F/6126229.html:text/html},
	Keywords = {Calibration, complex latent part-based model, Detectors, discriminative object detector, exemplar meta-data, exemplar-SVM, image classification, linear SVM classifier, nearest-neighbor approach, object detection, PASCAL VOC detection task, support vector machines, Three dimensional displays, Training, Vectors},
	Month = nov,
	Pages = {89--96},
	Title = {Ensemble of exemplar-{SVMs} for object detection and beyond},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCV.2011.6126229}}

@inproceedings{zhang_panocontext:_2014,
	Abstract = {The field-of-view of standard cameras is very small, which is one of the main reasons that contextual information is not as useful as it should be for object detection. To overcome this limitation, we advocate the use of 360$\,^{\circ}$ full-view panoramas in scene understanding, and propose a whole-room context model in 3D. For an input panorama, our method outputs 3D bounding boxes of the room and all major objects inside, together with their semantic categories. Our method generates 3D hypotheses based on contextual constraints and ranks the hypotheses holistically, combining both bottom-up and top-down context information. To train our model, we construct an annotated panorama dataset and reconstruct the 3D model from single-view using manual annotation. Experiments show that solely based on 3D context without any image region category classifier, we can achieve a comparable performance with the state-of-the-art object detector. This demonstrates that when the FOV is large, context is as powerful as object appearance. All data and source code are available online.},
	Author = {Zhang, Yinda and Song, Shuran and Tan, Ping and Xiao, Jianxiong},
	Booktitle = {Computer {Vision} -- {ECCV} 2014},
	Doi = {10.1007/978-3-319-10599-4_43},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/24ERN2BR/978-3-319-10599-4_43.html:text/html},
	Language = {en},
	Month = sep,
	Pages = {668--686},
	Publisher = {Springer, Cham},
	Shorttitle = {{PanoContext}},
	Title = {{PanoContext}: {A} {Whole}-{Room} 3D {Context} {Model} for {Panoramic} {Scene} {Understanding}},
	Url = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_43},
	Urldate = {2017-06-23},
	Year = {2014},
	Bdsk-Url-1 = {https://link.springer.com/chapter/10.1007/978-3-319-10599-4_43},
	Bdsk-Url-2 = {http://dx.doi.org/10.1007/978-3-319-10599-4_43}}

@article{ji_3d_2013,
	Abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
	Author = {Ji, S. and Xu, W. and Yang, M. and Yu, K.},
	Doi = {10.1109/TPAMI.2012.59},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/6C8S572B/6165309.html:text/html;IEEE Xplore Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/RP4AKFRD/Ji et al. - 2013 - 3D Convolutional Neural Networks for Human Action .pdf:application/pdf},
	Issn = {0162-8828},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {3D CNN model, 3D convolution, 3D convolutional neural networks, action recognition, airport surveillance videos, Algorithms, automated human action recognition, baseline methods, complex handcrafted features, Computational modeling, Computer architecture, convolutional neural networks, Decision Support Techniques, Deep learning, deep model, feature extraction, feature representation, gesture recognition, high-level features, image classification, Image Interpretation, Computer-Assisted, image motion analysis, image representation, Imaging, Three-Dimensional, Kernel, model combination, motion information encoding, Movement, neural nets, Neural Networks (Computer), Pattern Recognition, Automated, Solid modeling, spatial dimensions, spatiotemporal phenomena, Subtraction Technique, temporal dimensions, Three dimensional displays, Videos, video surveillance},
	Month = jan,
	Number = {1},
	Pages = {221--231},
	Title = {3D {Convolutional} {Neural} {Networks} for {Human} {Action} {Recognition}},
	Volume = {35},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TPAMI.2012.59}}

@misc{noauthor_marvin:_nodate,
	File = {Marvin\: Deep Learning in N Dimensions:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/T64RC668/marvin.is.html:text/html},
	Title = {Marvin: {Deep} {Learning} in {N} {Dimensions}},
	Url = {http://marvin.is/},
	Urldate = {2017-06-24},
	Bdsk-Url-1 = {http://marvin.is/}}

@article{wei_stc:_2016,
	Abstract = {Recently, significant improvement has been made on semantic object segmentation due to the development of deep convolutional neural networks (DCNNs). Training such a DCNN usually relies on a large number of images with pixel-level segmentation masks, and annotating these images is very costly in terms of both finance and human effort. In this paper, we propose a simple to complex (STC) framework in which only image-level annotations are utilized to learn DCNNs for semantic segmentation. Specifically, we first train an initial segmentation network called Initial-DCNN with the saliency maps of simple images (i.e., those with a single category of major object(s) and clean background). These saliency maps can be automatically obtained by existing bottom-up salient object detection techniques, where no supervision information is needed. Then, a better network called Enhanced-DCNN is learned with supervision from the predicted segmentation masks of simple images based on the Initial-DCNN as well as the image-level annotations. Finally, more pixel-level segmentation masks of complex images (two or more categories of objects with cluttered background), which are inferred by using Enhanced-DCNN and image-level annotations, are utilized as the supervision information to learn the Powerful-DCNN for semantic segmentation. Our method utilizes \$40\$K simple images from Flickr.com and 10K complex images from PASCAL VOC for step-wisely boosting the segmentation network. Extensive experimental results on PASCAL VOC 2012 segmentation benchmark well demonstrate the superiority of the proposed STC framework compared with other state-of-the-arts.},
	Annote = {Comment: To Appear in IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Author = {Wei, Yunchao and Liang, Xiaodan and Chen, Yunpeng and Shen, Xiaohui and Cheng, Ming-Ming and Feng, Jiashi and Zhao, Yao and Yan, Shuicheng},
	Doi = {10.1109/TPAMI.2016.2636150},
	File = {arXiv\:1509.03150 PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/3HXUPDB4/Wei et al. - 2016 - STC A Simple to Complex Framework for Weakly-supe.pdf:application/pdf;arXiv.org Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/GRZ8RIHM/1509.html:text/html},
	Issn = {0162-8828, 2160-9292},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	Keywords = {Computer Science - Computer Vision and Pattern Recognition},
	Note = {arXiv: 1509.03150},
	Pages = {1--1},
	Shorttitle = {{STC}},
	Title = {{STC}: {A} {Simple} to {Complex} {Framework} for {Weakly}-supervised {Semantic} {Segmentation}},
	Url = {http://arxiv.org/abs/1509.03150},
	Year = {2016},
	Bdsk-Url-1 = {http://arxiv.org/abs/1509.03150},
	Bdsk-Url-2 = {http://dx.doi.org/10.1109/TPAMI.2016.2636150}}

@inproceedings{bai_gift:_2016,
	Author = {Bai, Song and Bai, Xiang and Zhou, Zhichao and Zhang, Zhaoxiang and Jan Latecki, Longin},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/9F2MN4JN/Bai et al. - 2016 - GIFT A Real-Time and Scalable 3D Shape Search Eng.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/HZXT54D2/Bai_GIFT_A_Real-Time_CVPR_2016_paper.html:text/html},
	Pages = {5023--5032},
	Shorttitle = {{GIFT}},
	Title = {{GIFT}: {A} {Real}-{Time} and {Scalable} 3D {Shape} {Search} {Engine}},
	Url = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Bai_GIFT_A_Real-Time_CVPR_2016_paper.html},
	Urldate = {2017-06-24},
	Year = {2016},
	Bdsk-Url-1 = {http://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Bai_GIFT_A_Real-Time_CVPR_2016_paper.html}}

@article{fehr_covariance_2016,
	Abstract = {Processing 3D point cloud data is of primary interest in many areas of computer vision, including object grasping, robot navigation, and object recognition. The introduction of affordable RGB-D sensors has created a great interest in the computer vision community towards developing efficient algorithms for point cloud processing. Previously, capturing a point cloud required expensive specialized sensors such as lasers or dedicated range imaging devices; now, range data is readily available from low-cost sensors that provide easily extractable point clouds from a depth map. From here, an interesting challenge is to find different objects in the point cloud. Various descriptors have been introduced to match features in a point cloud. Cheap sensors are not necessarily designed to produce precise measurements, which means that the data is not as accurate as a point cloud provided from a laser or a dedicated range finder. Although some feature descriptors have been shown to be successful in recognizing objects from point clouds, there still exists opportunities for improvement. The aim of this paper is to introduce techniques from other fields, such as image processing, into 3D point cloud processing in order to improve rendering, classification, and recognition. Covariances have proven to be a success not only in image processing, but in other domains as well. This work develops the application of covariances in conjunction with 3D point cloud data.},
	Annote = {Could be very fast indeed. And Vitaliy could probably explain this until the day's end. May be far faster to implement in c++ too if the point cloud data is pre-processed enough},
	Author = {Fehr, Duc and Beksi, William J. and Zermas, Dimitris and Papanikolopoulos, Nikolaos},
	Doi = {10.1016/j.cviu.2015.06.008},
	File = {ScienceDirect Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/GDA6ADWF/Fehr et al. - 2016 - Covariance based point cloud descriptors for objec.pdf:application/pdf;ScienceDirect Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/XNKRIX8X/S1077314215001368.html:text/html},
	Issn = {1077-3142},
	Journal = {Computer Vision and Image Understanding},
	Keywords = {Classification, Colored point clouds, object recognition, RGB-D data},
	Month = jan,
	Pages = {80--93},
	Title = {Covariance based point cloud descriptors for object detection and recognition},
	Url = {http://www.sciencedirect.com/science/article/pii/S1077314215001368},
	Volume = {142},
	Year = {2016},
	Bdsk-Url-1 = {http://www.sciencedirect.com/science/article/pii/S1077314215001368},
	Bdsk-Url-2 = {http://dx.doi.org/10.1016/j.cviu.2015.06.008}}

@inproceedings{cheng_query_2015,
	Author = {Cheng, Yanhua and Cai, Rui and Zhang, Chi and Li, Zhiwei and Zhao, Xin and Huang, Kaiqi and Rui, Yong},
	File = {Full Text PDF:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/99M7UVHX/Cheng et al. - 2015 - Query Adaptive Similarity Measure for RGB-D Object.pdf:application/pdf;Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/DV9SRGWR/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.html:text/html},
	Pages = {145--153},
	Title = {Query {Adaptive} {Similarity} {Measure} for {RGB}-{D} {Object} {Recognition}},
	Url = {http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.html},
	Urldate = {2017-06-24},
	Year = {2015},
	Bdsk-Url-1 = {http://www.cv-foundation.org/openaccess/content_iccv_2015/html/Cheng_Query_Adaptive_Similarity_ICCV_2015_paper.html}}

@article{fischler_random_1981,
	Abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
	Author = {Fischler, Martin A. and Bolles, Robert C.},
	Doi = {10.1145/358669.358692},
	Issn = {0001-0782},
	Journal = {Commun. ACM},
	Keywords = {automated cartography, camera calibration, image matching, location determination, model fitting, scene analysis},
	Month = jun,
	Number = {6},
	Pages = {381--395},
	Shorttitle = {Random {Sample} {Consensus}},
	Title = {Random {Sample} {Consensus}: {A} {Paradigm} for {Model} {Fitting} with {Applications} to {Image} {Analysis} and {Automated} {Cartography}},
	Url = {http://doi.acm.org/10.1145/358669.358692},
	Volume = {24},
	Year = {1981},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/358669.358692},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/358669.358692}}

@inproceedings{golovinskiy_min-cut_2009,
	Abstract = {We present a min-cut based method of segmenting objects in point clouds. Given an object location, our method builds a k-nearest neighbors graph, assumes a background prior, adds hard foreground (and optionally background) constraints, and finds the min-cut to compute a foreground-background segmentation. Our method can be run fully automatically, or interactively with a user interface. We test our system on an outdoor urban scan, quantitatively evaluate our algorithm on a test set of about 1000 objects, and compare to several alternative approaches.},
	Author = {Golovinskiy, A. and Funkhouser, T.},
	Booktitle = {2009 {IEEE} 12th {International} {Conference} on {Computer} {Vision} {Workshops}, {ICCV} {Workshops}},
	Doi = {10.1109/ICCVW.2009.5457721},
	File = {IEEE Xplore Abstract Record:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/UC2GUN3B/5457721.html:text/html},
	Keywords = {Clouds, Computer graphics, foreground-background segmentation, graph theory, image segmentation, k-nearest neighbors graph, Land surface, Large-scale systems, Layout, min-cut based segmentation, object detection, object location, objects segmentation, optimisation, point clouds, Sampling methods, Surface cleaning, System testing, user interface},
	Month = sep,
	Pages = {39--46},
	Title = {Min-cut based segmentation of point clouds},
	Year = {2009},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICCVW.2009.5457721}}

@misc{noauthor_couchdb:_2017,
	Copyright = {Apache-2.0},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/RAZUPUFA/1.6.html:text/html},
	Month = jun,
	Note = {original-date: 2009-05-21T02:03:38Z},
	Publisher = {The Apache Software Foundation},
	Shorttitle = {couchdb},
	Title = {couchdb: {Apache} {CouchDB}},
	Url = {https://github.com/apache/couchdb},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/apache/couchdb}}

@misc{noauthor_geocouch:_2017,
	Copyright = {Apache-2.0},
	File = {Snapshot:/Users/LordNelson/Library/Application Support/Zotero/Profiles/vn3m5a3z.default/zotero/storage/DJTSP4W9/couchdb1.3.html:text/html},
	Month = jun,
	Note = {original-date: 2011-03-03T01:31:01Z},
	Publisher = {couchbase},
	Shorttitle = {geocouch},
	Title = {geocouch: {GeoCouch}, a spatial index for {CouchDB}},
	Url = {https://github.com/couchbase/geocouch},
	Year = {2017},
	Bdsk-Url-1 = {https://github.com/couchbase/geocouch}}

@book{bittner2002use,
	Author = {Bittner, Kurt},
	Publisher = {Addison-Wesley Longman Publishing Co., Inc.},
	Title = {Use case modeling},
	Year = {2002}}

@book{brennan2009guide,
	Author = {Brennan, Kevin and others},
	Publisher = {Iiba},
	Title = {A Guide to the Business Analysis Body of Knowledge},
	Year = {2009}}
